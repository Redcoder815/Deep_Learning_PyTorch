{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/32AttentionScoringFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "4z5JjtL0u_I1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[None, :]: This adds a new dimension at the beginning of the tensor, turning it into a row vector. So, [0, 1, 2, 3, 4] becomes [[0, 1, 2, 3, 4]]. This is crucial for broadcasting later.\n",
        "\n",
        "valid_len[:, None]: This takes your valid_len tensor (which contains the actual lengths of each sequence) and adds a new dimension at the end, turning it into a column vector. For example, if valid_len is torch.tensor([3, 2]) (meaning the first sequence has 3 valid items and the second has 2), this part would become [[3], [2]].\n",
        "\n",
        "<: This performs an element-wise comparison between the two tensors created in steps 2 and 3. Due to broadcasting rules in PyTorch, the row vector [[0, 1, 2, 3, 4]] and the column vector [[3], [2]] are expanded to match dimensions before the comparison.\n",
        "\n",
        "Let's walk through an example:\n",
        "\n",
        "Assume:\n",
        "\n",
        "maxlen = 5\n",
        "valid_len = torch.tensor([3, 2]) (meaning you have two sequences; the first has 3 valid elements, the second has 2).\n",
        "Step-by-step:\n",
        "\n",
        "The first part torch.arange(...) creates [0, 1, 2, 3, 4]. After [None, :], it becomes index_tensor = [[0, 1, 2, 3, 4]].\n",
        "The valid_len[:, None] part creates lengths_tensor = [[3], [2]].\n",
        "Now, we compare index_tensor < lengths_tensor:\n",
        "\n",
        "[[0, 1, 2, 3, 4]]\n",
        "<\n",
        "[[3],\n",
        " [2]]\n",
        "PyTorch broadcasts these. Conceptually, it expands them to:\n",
        "\n",
        "index_tensor becomes:\n",
        "[[0, 1, 2, 3, 4],\n",
        " [0, 1, 2, 3, 4]]\n",
        "lengths_tensor becomes:\n",
        "[[3, 3, 3, 3, 3],\n",
        " [2, 2, 2, 2, 2]]\n",
        "Performing the element-wise comparison (<) then yields:\n",
        "\n",
        "mask = [[True, True, True, False, False],   (0<3, 1<3, 2<3, 3<3 is False, 4<3 is False)\n",
        "        [True, True, False, False, False]]  (0<2, 1<2, 2<2 is False, 3<2 is False, 4<2 is False)\n",
        "This mask tensor tells you exactly which positions in each sequence (row) are valid (True) and which are padding/invalid (False)."
      ],
      "metadata": {
        "id": "yFpjOI8Iyc0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "bSmDLgNU0vW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line is used to set specific elements within a tensor X to a particular value, based on a boolean mask. The key part here is ~mask.\n",
        "\n",
        "mask: As we saw, the mask tensor contains True for valid elements and False for invalid (padded) elements. Continuing with our previous example, mask was:\n",
        "\n",
        "mask = [[True, True, True, False, False],\n",
        "        [True, True, False, False, False]]\n",
        "~mask: The tilde ~ operator inverts the boolean values in the mask tensor. So, True becomes False and False becomes True.\n",
        "\n",
        "Applying ~ to our mask example:\n",
        "\n",
        "~mask = [[False, False, False, True, True],\n",
        "         [False, False, True, True, True]]\n",
        "Now, True indicates the positions that were originally invalid (padded) in our sequence.\n",
        "\n",
        "X[~mask] = value: This operation uses the inverted mask (~mask) to select elements within the tensor X and then assigns value to only those selected elements. In other words, it sets all the invalid elements (where ~mask is True) in X to the specified value.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's assume you have a tensor X with some values, for instance:\n",
        "\n",
        "X = [[10, 11, 12, 13, 14],\n",
        "     [20, 21, 22, 23, 24]]\n",
        "And let's say value = -1e6 (a very small negative number often used to make masked elements become 0 after a softmax operation).\n",
        "\n",
        "Using our ~mask:\n",
        "\n",
        "~mask = [[False, False, False, True, True],\n",
        "         [False, False, True, True, True]]\n",
        "The operation X[~mask] = value will look at all the positions where ~mask is True and replace the corresponding elements in X with -1e6.\n",
        "\n",
        "So, X would become:\n",
        "\n",
        "X = [[10, 11, 12, -1e6, -1e6],\n",
        "     [20, 21, -1e6, -1e6, -1e6]]\n",
        "This effectively 'masks out' the invalid elements by setting them to a specific value, usually one that will have no impact (or a desired specific impact, like becoming 0) in subsequent calculations (like a softmax function)."
      ],
      "metadata": {
        "id": "lR5vAoFZ0wUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------"
      ],
      "metadata": {
        "id": "5SV5mY0J2eC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's apply valid_lens = torch.repeat_interleave(valid_lens, shape[1]):\n",
        "\n",
        "input is valid_lens = torch.tensor([5, 4])\n",
        "repeats is shape[1] = 3\n",
        "The operation will take the first element of valid_lens (which is 5) and repeat it 3 times: [5, 5, 5]. Then, it takes the second element of valid_lens (which is 4) and repeats it 3 times: [4, 4, 4].\n",
        "\n",
        "Combining these, the new valid_lens tensor will be:\n",
        "\n",
        "torch.tensor([5, 5, 5, 4, 4, 4])\n",
        "So, from an initial valid_lens of [5, 4] for a batch of 2, if each batch item contains 3 sequences (as implied by shape[1]=3), the valid_lens is expanded to [5, 5, 5, 4, 4, 4], now representing the valid lengths for each of the 2 * 3 = 6 sequences after X is potentially reshaped. This prepares the valid_lens to correspond element-wise to the flattened sequences that will be passed to _sequence_mask."
      ],
      "metadata": {
        "id": "--ZiBaLE2e_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masked Softmax Operation"
      ],
      "metadata": {
        "id": "tl3dBW0PIFeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        # On the last axis, replace masked elements with a very large negative\n",
        "        # value, whose exponentiation outputs 0\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ],
      "metadata": {
        "id": "_bOtYe7Zw6XN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNQXmBWh2yTj",
        "outputId": "132b5dfa-8fc6-4c16-cd36-5101162fdd47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4416, 0.5584, 0.0000, 0.0000],\n",
              "         [0.3445, 0.6555, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.2835, 0.4354, 0.2811, 0.0000],\n",
              "         [0.3568, 0.3621, 0.2811, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_sNMkQA3F_e",
        "outputId": "0c68611f-38bb-48f9-9cb8-177f88aa9dad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2025, 0.3446, 0.4530, 0.0000]],\n",
              "\n",
              "        [[0.5520, 0.4480, 0.0000, 0.0000],\n",
              "         [0.2307, 0.2830, 0.2738, 0.2126]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code executed successfully! The check_shape function confirmed that the shape of the output from torch.bmm(Q, K) is (2, 3, 6), which matches the expected shape. This means the batch matrix multiplication worked as intended. torch.bmm performs a batched matrix multiplication, where Q has shape (batch, n, m) and K has shape (batch, m, p), resulting in an output of shape (batch, n, p). In this case, Q is (2, 3, 4) and K is (2, 4, 6), so the result is (2, 3, 6).\n",
        "\n"
      ],
      "metadata": {
        "id": "dG3VzogW4sY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Matrix Multiplication"
      ],
      "metadata": {
        "id": "BPrFensSINJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_shape(tensor, expected_shape):\n",
        "    assert tensor.shape == expected_shape, f\"Expected shape {expected_shape}, but got {tensor.shape}\"\n",
        "    print(f\"Shape check passed: {tensor.shape} == {expected_shape}\")\n",
        "\n",
        "Q = torch.ones((2, 3, 4))\n",
        "K = torch.ones((2, 4, 6))\n",
        "check_shape(torch.bmm(Q, K), (2, 3, 6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS4uvpXD4TdT",
        "outputId": "2809c534-6440-4d4a-d9c0-14aeb1811f9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape check passed: torch.Size([2, 3, 6]) == (2, 3, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explain scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d) with an example.\n",
        "\n",
        "This line is a core component of the scaled dot-product attention mechanism, which is fundamental in many modern neural networks, especially transformers. It calculates a raw score for how much each query 'attends' to each key, and then scales these scores.\n",
        "\n",
        "Here's a breakdown of each part:\n",
        "\n",
        "queries: This is a tensor representing the queries. Its shape is typically (batch_size, num_queries, d), where d is the feature dimension.\n",
        "\n",
        "keys: This is a tensor representing the keys. Its shape is typically (batch_size, num_key_value_pairs, d).\n",
        "\n",
        "keys.transpose(1, 2): This operation swaps the second and third dimensions of the keys tensor. If keys is (batch_size, num_key_value_pairs, d), then keys.transpose(1, 2) becomes (batch_size, d, num_key_value_pairs). This is done to prepare the keys for matrix multiplication with the queries.\n",
        "\n",
        "torch.bmm(queries, keys.transpose(1, 2)): torch.bmm stands for \"batch matrix-matrix product\". It performs matrix multiplication for each batch independently. If:\n",
        "\n",
        "queries has shape (batch, N, D) (where N is num_queries)\n",
        "keys.transpose(1, 2) has shape (batch, D, M) (where M is num_key_value_pairs)\n",
        "The result, scores (before scaling), will have shape (batch, N, M). Each element scores[b, i, j] represents the dot product between the i-th query in batch b and the j-th key in batch b.\n",
        "d = queries.shape[-1]: This gets the last dimension of the queries tensor, which is the feature dimension d.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dkp1kqZw8Y1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaled Dot Product Attention"
      ],
      "metadata": {
        "id": "hdsMjDkOIVg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Shape of queries: (batch_size, no. of queries, d)\n",
        "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
        "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
        "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "pvIIDIQz6piB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 2))\n",
        "keys = torch.normal(0, 1, (2, 10, 2))\n",
        "values = torch.normal(0, 1, (2, 10, 4))\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = DotProductAttention(dropout=0.5)\n",
        "attention.eval()\n",
        "check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jICUdfqhF56s",
        "outputId": "e32f4e1f-c024-4818-e1ea-2dec1975303e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape check passed: torch.Size([2, 1, 4]) == (2, 1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additive Attention"
      ],
      "metadata": {
        "id": "B4ddkut5Ia3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explain features = queries.unsqueeze(2) + keys.unsqueeze(1) with an example.\n",
        "\n",
        "This line is performing an element-wise addition between two tensors (queries and keys) after expanding their dimensions. This dimension expansion is crucial for enabling a powerful feature in PyTorch called broadcasting, which allows tensors of different shapes to be combined under certain conditions.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "queries.unsqueeze(2):\n",
        "\n",
        "queries typically has a shape like (batch_size, num_queries, d). Let's assume (2, 1, 20) from your earlier example (2 batches, 1 query per batch, 20 features per query).\n",
        "The .unsqueeze(2) method adds a new dimension of size 1 at the specified index (index 2, which is the third dimension, as Python uses 0-based indexing).\n",
        "So, if queries was (batch_size, num_queries, d), after unsqueeze(2) it becomes (batch_size, num_queries, 1, d).\n",
        "Using our example: (2, 1, 20) becomes (2, 1, 1, 20).\n",
        "keys.unsqueeze(1):\n",
        "\n",
        "keys typically has a shape like (batch_size, num_key_value_pairs, d). Let's assume (2, 10, 20) (2 batches, 10 key-value pairs per batch, 20 features per key).\n",
        "The .unsqueeze(1) method adds a new dimension of size 1 at index 1 (the second dimension).\n",
        "So, if keys was (batch_size, num_key_value_pairs, d), after unsqueeze(1) it becomes (batch_size, 1, num_key_value_pairs, d).\n",
        "Using our example: (2, 10, 20) becomes (2, 1, 10, 20).\n",
        "+ (Broadcasting):\n",
        "\n",
        "Now you have two tensors with shapes: (2, 1, 1, 20) and (2, 1, 10, 20). (Note: In the AdditiveAttention class, the d for queries and keys are mapped to num_hiddens by W_q and W_k, so let's adjust our example's d to num_hiddens=8 as in the code).\n",
        "Let's refine the shapes based on the AdditiveAttention context:\n",
        "\n",
        "queries originally (batch_size, no. of queries, input_dim) -> after W_q(queries): (batch_size, no. of queries, num_hiddens). E.g., (2, 1, 8).\n",
        "\n",
        "queries.unsqueeze(2) becomes (batch_size, no. of queries, 1, num_hiddens). E.g., (2, 1, 1, 8).\n",
        "\n",
        "keys originally (batch_size, no. of key-value pairs, input_dim) -> after W_k(keys): (batch_size, no. of key-value pairs, num_hiddens). E.g., (2, 10, 8).\n",
        "\n",
        "keys.unsqueeze(1) becomes (batch_size, 1, no. of key-value pairs, num_hiddens). E.g., (2, 1, 10, 8).\n",
        "\n",
        "Now, when you add queries.unsqueeze(2) ((2, 1, 1, 8)) and keys.unsqueeze(1) ((2, 1, 10, 8)), PyTorch's broadcasting rules apply:\n",
        "\n",
        "It aligns the dimensions from right to left.\n",
        "If dimensions are equal, they match.\n",
        "If one dimension is 1, it's stretched (broadcast) to match the other.\n",
        "If dimensions are unequal and neither is 1, it's an error.\n",
        "In our example:\n",
        "\n",
        "Dimension 3 (features): 8 and 8 - Match.\n",
        "Dimension 2 (keys/queries placeholder): 1 and 10 - The 1 is broadcast to 10.\n",
        "Dimension 1 (queries/keys placeholder): 1 and 1 - Match (or 1 is broadcast to 1).\n",
        "Dimension 0 (batch): 2 and 2 - Match.\n",
        "The resulting features tensor will have the shape of the broadcasted dimensions, which is (batch_size, num_queries, num_key_value_pairs, num_hiddens). In our example, (2, 1, 10, 8).\n",
        "\n",
        "What this achieves:\n",
        "\n",
        "This operation effectively creates a tensor where each query's representation is combined element-wise with every key's representation. For instance, features[b, i, j, :] would contain the element-wise sum of the i-th query's features and the j-th key's features, for batch b.\n",
        "\n",
        "This is a common pattern in attention mechanisms, particularly additive attention, to prepare a tensor where every possible query-key interaction is explicitly represented, which then gets passed through a non-linear activation (like torch.tanh) before being projected to a single score by self.w_v."
      ],
      "metadata": {
        "id": "VSTR7VUgOXNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------"
      ],
      "metadata": {
        "id": "-4yznfsjOdXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you have Q as (Q1) and K as (K1, K2, K3), the operation effectively computes (Q1+K1, Q1+K2, Q1+K3). If Q was (Q1, Q2) it would be ((Q1+K1, Q1+K2, Q1+K3), (Q2+K1, Q2+K2, Q2+K3)) and so on. This creates all possible interaction pairs, which is a key step in additive attention."
      ],
      "metadata": {
        "id": "wMVc2aYvOeWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------"
      ],
      "metadata": {
        "id": "rKC9eGcJPn3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explain squeeze(-1) with a clear example.\n",
        "\n",
        "In PyTorch (and similar libraries), the .squeeze() method is used to remove dimensions of size 1 from a tensor. When you specify an argument like -1, you're telling it to only remove dimensions of size 1 at that specific position.\n",
        "\n",
        "squeeze(): Without any arguments, squeeze() will remove all dimensions that have a size of 1. For example, a tensor with shape (1, 3, 1, 4, 1) would become (3, 4).\n",
        "\n",
        "squeeze(-1): When you pass -1 as an argument, it means \"remove the last dimension if its size is 1\". If the last dimension is not 1, it does nothing.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you have a tensor named my_tensor with the following shape:\n",
        "\n",
        "my_tensor.shape = (2, 1, 3, 1, 5, 1)\n",
        "\n",
        "Let's see what happens with squeeze(-1):\n",
        "\n",
        "my_tensor.squeeze(-1): This checks the last dimension. Its size is 1. So, this dimension is removed.\n",
        "The new shape becomes: (2, 1, 3, 1, 5)\n",
        "Now, let's take the result from that operation and apply squeeze(-1) again:\n",
        "\n",
        "my_tensor_after_first_squeeze.squeeze(-1): This checks the last dimension, which now has a size of 5. Since it's not 1, this squeeze(-1) operation does nothing.\n",
        "The shape remains: (2, 1, 3, 1, 5)\n",
        "Why is squeeze(-1) often used in attention mechanisms?\n",
        "\n",
        "In the AdditiveAttention example we just discussed, the self.w_v linear layer was defined as nn.LazyLinear(1, bias=False). This means it transforms the input features into a single output value. So, if the input to self.w_v was (batch_size, num_queries, num_key_value_pairs, num_hiddens), the output would be (batch_size, num_queries, num_key_value_pairs, 1).\n",
        "\n",
        "This final 1 in the dimension is often redundant. It's just a single number, so having it wrapped in an extra dimension of size 1 doesn't add much meaning and can sometimes complicate subsequent operations. squeeze(-1) efficiently removes this unnecessary dimension, making the scores tensor have a cleaner shape like (batch_size, num_queries, num_key_value_pairs), which is more intuitive for representing the attention scores between each query and each key-value pair."
      ],
      "metadata": {
        "id": "CQJPOHRMPo6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    \"\"\"Additive attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
        "        super(AdditiveAttention, self).__init__(**kwargs)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
        "        self.w_v = nn.LazyLinear(1, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
        "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
        "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "        features = torch.tanh(features)\n",
        "        # There is only one output of self.w_v, so we remove the last\n",
        "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
        "        # no. of queries, no. of key-value pairs)\n",
        "        scores = self.w_v(features).squeeze(-1)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
        "        # dimension)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "H3ULf7C5IdNQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = torch.normal(0, 1, (2, 1, 20))\n",
        "\n",
        "attention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n",
        "attention.eval()\n",
        "check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSZCEN81LzUU",
        "outputId": "01e3bb6d-9638-4a5a-b8b1-44d667b3b9f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape check passed: torch.Size([2, 1, 4]) == (2, 1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example for queries after W_q and unsqueeze(2)\n",
        "# Shape: (batch_size, num_queries, 1, num_hiddens)\n",
        "# Let's simplify with batch_size=1, num_queries=1, num_hiddens=2\n",
        "queries_expanded = torch.tensor([[[[1.0, 2.0]]]]) # Shape (1, 1, 1, 2)\n",
        "print(f\"Shape of queries_expanded: {queries_expanded.shape}\\n{queries_expanded}\")\n",
        "\n",
        "# Example for keys after W_k and unsqueeze(1)\n",
        "# Shape: (batch_size, 1, num_key_value_pairs, num_hiddens)\n",
        "# Let's simplify with batch_size=1, num_key_value_pairs=3, num_hiddens=2\n",
        "keys_expanded = torch.tensor([[[ # (batch_size, 1, num_key_value_pairs, num_hiddens)\n",
        "    [10.0, 20.0],\n",
        "    [30.0, 40.0],\n",
        "    [50.0, 60.0]\n",
        "]]]) # Corrected Shape (1, 1, 3, 2)\n",
        "print(f\"Shape of keys_expanded: {keys_expanded.shape}\\n{keys_expanded}\")\n",
        "\n",
        "# Perform element-wise addition with broadcasting\n",
        "features = queries_expanded + keys_expanded\n",
        "\n",
        "print(f\"\\nShape of result (features): {features.shape}\\n{features}\")\n",
        "\n",
        "# Let's verify the broadcasting for one example (Q1 + K1, Q1 + K2, Q1 + K3)\n",
        "# Conceptually, Q1 = [1.0, 2.0]\n",
        "# K1 = [10.0, 20.0]\n",
        "# K2 = [30.0, 40.0]\n",
        "# K3 = [50.0, 60.0]\n",
        "\n",
        "# Expected result for the first query:\n",
        "# Q1 + K1 = [1.0+10.0, 2.0+20.0] = [11.0, 22.0]\n",
        "# Q1 + K2 = [1.0+30.0, 2.0+40.0] = [31.0, 42.0]\n",
        "# Q1 + K3 = [1.0+50.0, 2.0+60.0] = [51.0, 62.0]"
      ],
      "metadata": {
        "id": "JwD7GEz-RDyG",
        "outputId": "a703ab2b-cf9c-4767-c261-2d9446d4d77a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of queries_expanded: torch.Size([1, 1, 1, 2])\n",
            "tensor([[[[1., 2.]]]])\n",
            "Shape of keys_expanded: torch.Size([1, 1, 3, 2])\n",
            "tensor([[[[10., 20.],\n",
            "          [30., 40.],\n",
            "          [50., 60.]]]])\n",
            "\n",
            "Shape of result (features): torch.Size([1, 1, 3, 2])\n",
            "tensor([[[[11., 22.],\n",
            "          [31., 42.],\n",
            "          [51., 62.]]]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
