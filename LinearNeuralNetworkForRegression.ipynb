{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/LinearNeuralNetworkForRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "gzRdY8Aj6-5U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e1cd391"
      },
      "source": [
        "class LinearRegressionScratch(nn.Module):\n",
        "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
        "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.lr = lr # Store lr as an instance attribute\n",
        "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
        "        self.b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "      return torch.matmul(X, self.w) + self.b\n",
        "\n",
        "    def loss(self, y_hat, y):\n",
        "      l = (y_hat-y)**2 / 2\n",
        "      return l.mean()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      return SGD([self.w, self.b], self.lr)\n",
        "\n",
        "    def prepare_batch(self, batch):\n",
        "      return batch"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62e49dd8"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# 1. Define the true weight vector w and bias b\n",
        "true_w = torch.tensor([2, -3.4]).reshape(-1, 1)\n",
        "true_b = torch.tensor([4.2])\n",
        "\n",
        "# Define dataset parameters\n",
        "num_samples = 1000\n",
        "num_features = 2\n",
        "batch_size = 32\n",
        "\n",
        "# 2. Generate a synthetic dataset X of features and y of labels\n",
        "X = torch.randn(num_samples, num_features)\n",
        "y = torch.matmul(X, true_w) + true_b + torch.randn(num_samples, 1) * 0.01\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a798bc0"
      },
      "source": [
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "# 4. Split the TensorDataset into training and validation sets\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# 5. Create DataLoader instances for both the training and validation sets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8567f678"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_dataloader, val_dataloader=None, max_epochs=10):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.max_epochs = max_epochs\n",
        "        self.optim = model.configure_optimizers()\n",
        "\n",
        "    def fit(self):\n",
        "        for epoch in range(self.max_epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "            for X, y in self.train_dataloader:\n",
        "                # Forward pass\n",
        "                y_hat = self.model(X)\n",
        "                # Calculate loss\n",
        "                loss = self.model.loss(y_hat, y)\n",
        "\n",
        "                # Backpropagation\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                # Optimization step\n",
        "                self.optim.step()\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
        "            print(f\"Epoch {epoch + 1}/{self.max_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            # Validation loop (optional)\n",
        "            if self.val_dataloader is not None:\n",
        "                self.model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for X_val, y_val in self.val_dataloader:\n",
        "                        y_hat_val = self.model(X_val)\n",
        "                        val_loss = self.model.loss(y_hat_val, y_val)\n",
        "                        total_val_loss += val_loss.item()\n",
        "                avg_val_loss = total_val_loss / len(self.val_dataloader)\n",
        "                print(f\"Epoch {epoch + 1}/{self.max_epochs}, Validation Loss: {avg_val_loss:.4f}\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba399052"
      },
      "source": [
        "model = LinearRegressionScratch(num_features, lr=0.03)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3742400b"
      },
      "source": [
        "max_epochs = 10\n",
        "trainer = Trainer(model, train_dataloader, val_dataloader, max_epochs=max_epochs)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9a0b02",
        "outputId": "0ae712bd-47b7-4d0b-b2ce-db9c82ead566"
      },
      "source": [
        "trainer.fit()\n",
        "print(\"Model training initiated.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 8.6898\n",
            "Epoch 1/10, Validation Loss: 4.5307\n",
            "Epoch 2/10, Training Loss: 1.9897\n",
            "Epoch 2/10, Validation Loss: 1.0820\n",
            "Epoch 3/10, Training Loss: 0.4675\n",
            "Epoch 3/10, Validation Loss: 0.2632\n",
            "Epoch 4/10, Training Loss: 0.1127\n",
            "Epoch 4/10, Validation Loss: 0.0649\n",
            "Epoch 5/10, Training Loss: 0.0277\n",
            "Epoch 5/10, Validation Loss: 0.0162\n",
            "Epoch 6/10, Training Loss: 0.0069\n",
            "Epoch 6/10, Validation Loss: 0.0040\n",
            "Epoch 7/10, Training Loss: 0.0018\n",
            "Epoch 7/10, Validation Loss: 0.0010\n",
            "Epoch 8/10, Training Loss: 0.0005\n",
            "Epoch 8/10, Validation Loss: 0.0003\n",
            "Epoch 9/10, Training Loss: 0.0002\n",
            "Epoch 9/10, Validation Loss: 0.0001\n",
            "Epoch 10/10, Training Loss: 0.0001\n",
            "Epoch 10/10, Validation Loss: 0.0001\n",
            "Model training initiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "373ed341"
      },
      "source": [
        "class SGD():\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    def __init__(self, params, lr):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        for param in self.params:\n",
        "            # Ensure param.grad is not None before performing operation\n",
        "            if param.grad is not None:\n",
        "                param.data -= self.lr * param.grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.zero_()"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}