{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/19NiN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import data\n",
        "from torchvision import datasets\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "MIYlSlGrqITF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9hJxwzcrOTk",
        "outputId": "d52ab26c-c5be-4dde-de68-5b2cf1bd1b1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nin_block(out_channels, kernel_size, strides, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())"
      ],
      "metadata": {
        "id": "pw5Y8-ReqNbA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nn.AdaptiveAvgPool2d layer in PyTorch is a type of pooling operation that automatically adjusts the kernel size and stride to output a tensor of a specified size, regardless of the input size. It's commonly used at the end of convolutional neural networks to prepare feature maps for the fully connected layers.\n",
        "\n",
        "Specifically, nn.AdaptiveAvgPool2d((1, 1)) will take any input size and output a tensor where the spatial dimensions (height and width) are both 1. This effectively computes the average of each feature map across all its spatial dimensions, resulting in a single value per feature map. This is useful for classification tasks, as it provides a fixed-size representation of the features before passing them to a linear layer for final classification."
      ],
      "metadata": {
        "id": "4G_dZXpRsPr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NiN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nin_block(96, kernel_size=11, strides=4, padding=0),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(256, kernel_size=5, strides=1, padding=2),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(384, kernel_size=3, strides=1, padding=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.Dropout(0.5),\n",
        "            nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten())\n",
        "    def forward(self, X):\n",
        "      return self.net(X)"
      ],
      "metadata": {
        "id": "5JGl30A0qqka"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NiN()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-HSl0cMq5mH",
        "outputId": "8a4b6100-0c56-470e-97bf-e42c2d2970d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NiN(\n",
              "  (net): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): LazyConv2d(0, 96, kernel_size=(11, 11), stride=(4, 4))\n",
              "      (1): ReLU()\n",
              "      (2): LazyConv2d(0, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (3): ReLU()\n",
              "      (4): LazyConv2d(0, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (2): Sequential(\n",
              "      (0): LazyConv2d(0, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (1): ReLU()\n",
              "      (2): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (3): ReLU()\n",
              "      (4): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): LazyConv2d(0, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "      (2): LazyConv2d(0, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (3): ReLU()\n",
              "      (4): LazyConv2d(0, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU()\n",
              "    )\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Dropout(p=0.5, inplace=False)\n",
              "    (7): Sequential(\n",
              "      (0): LazyConv2d(0, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "      (2): LazyConv2d(0, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (3): ReLU()\n",
              "      (4): LazyConv2d(0, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU()\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (9): Flatten(start_dim=1, end_dim=-1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256"
      ],
      "metadata": {
        "id": "M8DrOmlJtsEL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Transform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "metadata": {
        "id": "0HZvn_aptwFY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The num_workers parameter in PyTorch's DataLoader specifies how many subprocesses to use for data loading.\n",
        "\n",
        "When num_workers is set to 0 (the default), data will be loaded in the main process. This can be slow, especially for large datasets or complex data transformations, as the main process has to do both data loading and model training.\n",
        "When num_workers is set to a value greater than 0 (like 2 in your code), the DataLoader will use that many separate processes to fetch and preprocess the data in parallel. This can significantly speed up training by ensuring that the CPU is busy preparing the next batch of data while the GPU (or CPU) is busy training the model on the current batch.\n",
        "Setting num_workers=2 means that two worker processes will be launched to load the data concurrently, potentially making the data loading more efficient."
      ],
      "metadata": {
        "id": "oh8oMQFsvdNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = datasets.FashionMNIST(root=\"../data\", train=True, transform=Transform, download=True)\n",
        "mnist_val = datasets.FashionMNIST(root=\"../data\", train=False, transform=Transform, download=True)\n",
        "\n",
        "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=2)\n",
        "val_iter = data.DataLoader(mnist_val, batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "k311YNqYuGQi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "lmSz-f0mvr6h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 3"
      ],
      "metadata": {
        "id": "WY9qP7kowXBn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(max_epochs):\n",
        "  model.train()\n",
        "  train_loss_sum, train_accuracy_sum, n = 0.0, 0.0, 0\n",
        "  for images, labels in train_iter:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    y_pred = model(images)\n",
        "    l = criterion(y_pred, labels)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    train_loss_sum += l\n",
        "    predicted_labels = torch.argmax(y_pred, dim=1)\n",
        "    train_accuracy_sum += (predicted_labels == labels).float().sum()\n",
        "    n += labels.numel()\n",
        "\n",
        "  model.eval()\n",
        "  test_accuracy_sum, test_n = 0.0, 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in val_iter:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      y_pred = model(images)\n",
        "      predicted_labels = torch.argmax(y_pred, dim=1)\n",
        "      test_accuracy_sum += (predicted_labels == labels).float().sum()\n",
        "      test_n += labels.numel()\n",
        "  test_accuracy = test_accuracy_sum / test_n\n",
        "  print(f'Epoch {epoch + 1}, Loss: {train_loss_sum / n:.4f}, Train Accuracy: {train_accuracy_sum / n:.4f}, Validation Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0PndZBWxE6m",
        "outputId": "4bb43a70-5962-418f-f54d-e50602a81d93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0065, Train Accuracy: 0.3827, Validation Accuracy: 0.6803\n",
            "Epoch 2, Loss: 0.0028, Train Accuracy: 0.7430, Validation Accuracy: 0.7836\n",
            "Epoch 3, Loss: 0.0021, Train Accuracy: 0.7994, Validation Accuracy: 0.8220\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}