{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/26GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "kwoMJH4T3QDn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "class TimeMachine():\n",
        "    def __init__(self, batch_size = 2, num_steps = 10, num_train=10000, num_val=5000):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_steps = num_steps\n",
        "        self.num_train = num_train\n",
        "        self.num_val = num_val\n",
        "        corpus, self.vocab = self.build(self._download())\n",
        "        array = torch.tensor([corpus[i:i+num_steps+1]\n",
        "                            for i in range(len(corpus)-num_steps)])\n",
        "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
        "\n",
        "    def _download(self):\n",
        "        url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.text\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return list(text)\n",
        "\n",
        "    def build(self, raw_text, vocab=None):\n",
        "        tokens = self._tokenize(self._preprocess(raw_text))\n",
        "        if vocab is None: vocab = Vocab(tokens)\n",
        "        corpus = [vocab[token] for token in tokens]\n",
        "        return corpus, vocab\n",
        "\n",
        "    def get_tensorloader(self, data_arrays, train, i):\n",
        "        # This is a placeholder, actual implementation might vary based on inheritance.\n",
        "        features = data_arrays[0][i]\n",
        "        labels = data_arrays[1][i]\n",
        "        dataset = TensorDataset(features, labels)\n",
        "        dataloader = DataLoader(dataset, self.batch_size, shuffle=train)\n",
        "        return dataloader\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        idx = slice(0, self.num_train) if train else slice(\n",
        "            self.num_train, self.num_train + self.num_val)\n",
        "        return self.get_tensorloader([self.X, self.Y], train, idx)"
      ],
      "metadata": {
        "id": "Gpg7RRE83Z6n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set([''] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['']"
      ],
      "metadata": {
        "id": "MfY0lcNn3gmY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLMScratch(nn.Module):\n",
        "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
        "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.rnn = rnn # Store the RNN instance\n",
        "        self.vocab_size = vocab_size\n",
        "        self.loss = nn.CrossEntropyLoss() # Initialize the loss function\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        self.W_hq = nn.Parameter(\n",
        "            torch.randn(\n",
        "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
        "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        # self.plot('ppl', torch.exp(l), train=True) # Removed as self.plot is undefined\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        # self.plot('ppl', torch.exp(l), train=False) # Removed as self.plot is undefined\n",
        "\n",
        "    def one_hot(self, X):\n",
        "        # Output shape: (num_steps, batch_size, vocab_size)\n",
        "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "\n",
        "    def output_layer(self, rnn_outputs):\n",
        "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
        "        return torch.stack(outputs, 1)\n",
        "\n",
        "    def forward(self, X, state=None):\n",
        "        embs = self.one_hot(X)\n",
        "        rnn_outputs, _ = self.rnn(embs, state)\n",
        "        return self.output_layer(rnn_outputs)\n",
        "\n",
        "    def predict(self, prefix, num_preds, vocab, device=None):\n",
        "        state, outputs = None, [vocab[prefix[0]]]\n",
        "        for i in range(len(prefix) + num_preds - 1):\n",
        "            X = torch.tensor([[outputs[-1]]], device=device)\n",
        "            embs = self.one_hot(X)\n",
        "            rnn_outputs, state = self.rnn(embs, state)\n",
        "            if i < len(prefix) - 1:  # Warm-up period\n",
        "                outputs.append(vocab[prefix[i + 1]])\n",
        "            else:  # Predict num_preds steps\n",
        "                Y = self.output_layer(rnn_outputs)\n",
        "                outputs.append(int(Y.argmax(axis=2).reshape(1)))\n",
        "        return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "717aJfcA3mZT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUScratch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens # Stored as instance variable\n",
        "        self.sigma = sigma # Store sigma as an instance attribute\n",
        "\n",
        "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
        "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
        "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
        "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
        "\n",
        "    def forward(self, inputs, H=None):\n",
        "        if H is None:\n",
        "            # Initial state with shape: (batch_size, num_hiddens)\n",
        "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +\n",
        "                            torch.matmul(H, self.W_hz) + self.b_z)\n",
        "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
        "                            torch.matmul(H, self.W_hr) + self.b_r)\n",
        "            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\n",
        "                               torch.matmul(R * H, self.W_hh) + self.b_h)\n",
        "            H = Z * H + (1 - Z) * H_tilde\n",
        "            outputs.append(H)\n",
        "        return outputs, H"
      ],
      "metadata": {
        "id": "YtTydbe53sMZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = TimeMachine(batch_size=1024, num_steps=32)\n",
        "gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
        "model = RNNLMScratch(gru, vocab_size=len(data.vocab), lr=0.01)"
      ],
      "metadata": {
        "id": "sX7YpMcx4NPi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'it has a time'\n",
        "num_preds = 50\n",
        "predicted_text = model.predict(prefix, num_preds, data.vocab, device=None)\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mqopePZ4vzw",
        "outputId": "e4bfc226-dfe1-426c-fe99-968038b41072"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it has a timeukyacjalllntuzrrlllntuzrrlllntuzrrlllntuzrrlllntuz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concise implementation"
      ],
      "metadata": {
        "id": "OGfYi-RL5Ciq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The swapaxes(0, 1) operation in the output_layer function is used to reorder the dimensions of the tensor. Let's break it down:\n",
        "\n",
        "Tensor Shape Before swapaxes: After self.linear(hiddens) is applied, the tensor's shape is typically (sequence_length, batch_size, vocab_size). This means the first dimension represents the time steps (or sequence length), the second is the batch size, and the third is the vocabulary size (logits for each word).\n",
        "\n",
        "What swapaxes(0, 1) Does: It swaps the dimension at index 0 with the dimension at index 1. So, (sequence_length, batch_size, vocab_size) becomes (batch_size, sequence_length, vocab_size).\n",
        "\n",
        "Why it's Needed: This reordering is crucial for compatibility with PyTorch's nn.CrossEntropyLoss. When you flatten the target labels Y (which start as (batch_size, sequence_length)) into a 1D tensor (batch_size * sequence_length), the loss function expects the model's output (logits) to also be structured such that when it's flattened to (batch_size * sequence_length, vocab_size), the elements align correctly. By having the batch dimension first in (batch_size, sequence_length, vocab_size), the subsequent flattening output.reshape(-1, output.shape[-1]) correctly produces (batch_size * sequence_length, vocab_size), aligning perfectly with the flattened target labels. This ensures that the loss is calculated for the correct predictions corresponding to their respective targets within each sequence and batch."
      ],
      "metadata": {
        "id": "i51rjYvy_nEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLM(RNNLMScratch):\n",
        "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
        "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
        "        super().__init__(rnn, vocab_size, lr) # Call parent's __init__ to set lr\n",
        "\n",
        "    def init_params(self):\n",
        "        # For RNNLM, init_params is overridden by LazyLinear initialization\n",
        "        # LazyLinear handles weight initialization automatically\n",
        "        self.linear = nn.LazyLinear(self.vocab_size) # This should be here, not in __init__ for LazyLinear\n",
        "\n",
        "    def output_layer(self, hiddens):\n",
        "        return self.linear(hiddens).swapaxes(0, 1)"
      ],
      "metadata": {
        "id": "ZCpH2cth5FA5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        super().__init__()\n",
        "        # Removed self.save_hyperparameters() as it's not standard nn.Module\n",
        "        self.num_hiddens = num_hiddens # Store num_hiddens as an instance variable\n",
        "        self.rnn = nn.GRU(num_inputs, num_hiddens)\n",
        "\n",
        "    def forward(self, inputs, state=None):\n",
        "        # nn.GRU expects input shape (seq_len, batch_size, input_size)\n",
        "        # and returns output (seq_len, batch_size, num_hiddens) and h_n (1, batch_size, num_hiddens)\n",
        "        # The predict method sends (1, 1, vocab_size) as embs.\n",
        "        # Permute to (batch_size, seq_len, input_size) for nn.GRU, then back.\n",
        "        inputs = inputs.permute(1, 0, 2) # Change from (num_steps, batch_size, vocab_size) to (batch_size, num_steps, vocab_size) if needed\n",
        "        outputs, state = self.rnn(inputs, state)\n",
        "        outputs = outputs.permute(1, 0, 2) # Change back if needed\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "3qfCYJuU5Hqt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru = GRU(num_inputs=len(data.vocab), num_hiddens=128)\n",
        "model = RNNLM(gru, vocab_size=len(data.vocab), lr=0.01)"
      ],
      "metadata": {
        "id": "Tc0L7dNW5qJf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, num_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # Check if a CUDA-enabled GPU is available, otherwise use the CPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl_total, num_tokens = 0, 0\n",
        "        model.train() # Set the model to training mode\n",
        "        for X, Y in data.get_dataloader(train=True):\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            # Ensure Y has the correct shape for CrossEntropyLoss (batch_size * sequence_length)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            Y = Y.reshape(-1)\n",
        "            loss = model.loss(output, Y)\n",
        "            loss.backward()\n",
        "            # Clip gradients to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ppl_total += torch.exp(loss) * Y.numel()\n",
        "                num_tokens += Y.numel()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Perplexity: {ppl_total / num_tokens:.2f}')"
      ],
      "metadata": {
        "id": "-Epc01MX8w5T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "num_epochs = 100\n",
        "train(model, data, num_epochs, lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be8sF13o805b",
        "outputId": "f14c9931-64cd-4205-89e0-895c1d3d64a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Perplexity: 26.52\n",
            "Epoch 2, Perplexity: 20.57\n",
            "Epoch 3, Perplexity: 17.63\n",
            "Epoch 4, Perplexity: 16.87\n",
            "Epoch 5, Perplexity: 16.47\n",
            "Epoch 6, Perplexity: 16.11\n",
            "Epoch 7, Perplexity: 15.70\n",
            "Epoch 8, Perplexity: 15.23\n",
            "Epoch 9, Perplexity: 14.70\n",
            "Epoch 10, Perplexity: 14.12\n",
            "Epoch 11, Perplexity: 13.53\n",
            "Epoch 12, Perplexity: 12.95\n",
            "Epoch 13, Perplexity: 12.42\n",
            "Epoch 14, Perplexity: 11.97\n",
            "Epoch 15, Perplexity: 11.59\n",
            "Epoch 16, Perplexity: 11.29\n",
            "Epoch 17, Perplexity: 11.05\n",
            "Epoch 18, Perplexity: 10.85\n",
            "Epoch 19, Perplexity: 10.68\n",
            "Epoch 20, Perplexity: 10.55\n",
            "Epoch 21, Perplexity: 10.43\n",
            "Epoch 22, Perplexity: 10.33\n",
            "Epoch 23, Perplexity: 10.25\n",
            "Epoch 24, Perplexity: 10.18\n",
            "Epoch 25, Perplexity: 10.12\n",
            "Epoch 26, Perplexity: 10.06\n",
            "Epoch 27, Perplexity: 10.02\n",
            "Epoch 28, Perplexity: 9.98\n",
            "Epoch 29, Perplexity: 9.94\n",
            "Epoch 30, Perplexity: 9.91\n",
            "Epoch 31, Perplexity: 9.88\n",
            "Epoch 32, Perplexity: 9.86\n",
            "Epoch 33, Perplexity: 9.83\n",
            "Epoch 34, Perplexity: 9.81\n",
            "Epoch 35, Perplexity: 9.79\n",
            "Epoch 36, Perplexity: 9.77\n",
            "Epoch 37, Perplexity: 9.76\n",
            "Epoch 38, Perplexity: 9.74\n",
            "Epoch 39, Perplexity: 9.73\n",
            "Epoch 40, Perplexity: 9.71\n",
            "Epoch 41, Perplexity: 9.70\n",
            "Epoch 42, Perplexity: 9.69\n",
            "Epoch 43, Perplexity: 9.68\n",
            "Epoch 44, Perplexity: 9.67\n",
            "Epoch 45, Perplexity: 9.66\n",
            "Epoch 46, Perplexity: 9.65\n",
            "Epoch 47, Perplexity: 9.64\n",
            "Epoch 48, Perplexity: 9.63\n",
            "Epoch 49, Perplexity: 9.63\n",
            "Epoch 50, Perplexity: 9.62\n",
            "Epoch 51, Perplexity: 9.61\n",
            "Epoch 52, Perplexity: 9.61\n",
            "Epoch 53, Perplexity: 9.60\n",
            "Epoch 54, Perplexity: 9.60\n",
            "Epoch 55, Perplexity: 9.59\n",
            "Epoch 56, Perplexity: 9.58\n",
            "Epoch 57, Perplexity: 9.58\n",
            "Epoch 58, Perplexity: 9.57\n",
            "Epoch 59, Perplexity: 9.57\n",
            "Epoch 60, Perplexity: 9.56\n",
            "Epoch 61, Perplexity: 9.56\n",
            "Epoch 62, Perplexity: 9.56\n",
            "Epoch 63, Perplexity: 9.55\n",
            "Epoch 64, Perplexity: 9.55\n",
            "Epoch 65, Perplexity: 9.55\n",
            "Epoch 66, Perplexity: 9.54\n",
            "Epoch 67, Perplexity: 9.54\n",
            "Epoch 68, Perplexity: 9.54\n",
            "Epoch 69, Perplexity: 9.53\n",
            "Epoch 70, Perplexity: 9.53\n",
            "Epoch 71, Perplexity: 9.53\n",
            "Epoch 72, Perplexity: 9.53\n",
            "Epoch 73, Perplexity: 9.52\n",
            "Epoch 74, Perplexity: 9.52\n",
            "Epoch 75, Perplexity: 9.52\n",
            "Epoch 76, Perplexity: 9.52\n",
            "Epoch 77, Perplexity: 9.52\n",
            "Epoch 78, Perplexity: 9.51\n",
            "Epoch 79, Perplexity: 9.51\n",
            "Epoch 80, Perplexity: 9.51\n",
            "Epoch 81, Perplexity: 9.51\n",
            "Epoch 82, Perplexity: 9.51\n",
            "Epoch 83, Perplexity: 9.50\n",
            "Epoch 84, Perplexity: 9.50\n",
            "Epoch 85, Perplexity: 9.50\n",
            "Epoch 86, Perplexity: 9.50\n",
            "Epoch 87, Perplexity: 9.50\n",
            "Epoch 88, Perplexity: 9.49\n",
            "Epoch 89, Perplexity: 9.50\n",
            "Epoch 90, Perplexity: 9.49\n",
            "Epoch 91, Perplexity: 9.49\n",
            "Epoch 92, Perplexity: 9.49\n",
            "Epoch 93, Perplexity: 9.49\n",
            "Epoch 94, Perplexity: 9.49\n",
            "Epoch 95, Perplexity: 9.49\n",
            "Epoch 96, Perplexity: 9.48\n",
            "Epoch 97, Perplexity: 9.48\n",
            "Epoch 98, Perplexity: 9.48\n",
            "Epoch 99, Perplexity: 9.48\n",
            "Epoch 100, Perplexity: 9.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.predict('it has', 20, data.vocab, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_AFPWxF09G3y",
        "outputId": "c9bcff9e-b355-4185-f3c6-2af572f81192"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it has the the the the the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}