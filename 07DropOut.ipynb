{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/07DropOut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "lpoQaQAvWO-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dropout_layer(X, dropout):\n",
        "    assert 0 <= dropout <= 1\n",
        "    if dropout == 1: return torch.zeros_like(X)\n",
        "    mask = (torch.rand(X.shape) > dropout).float()\n",
        "    return mask * X / (1.0 - dropout)"
      ],
      "metadata": {
        "id": "Owc0p17OWVzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
        "print('dropout_p = 0:', dropout_layer(X, 0))\n",
        "print('dropout_p = 0.5:', dropout_layer(X, 0.5))\n",
        "print('dropout_p = 1:', dropout_layer(X, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8eG6BzwWY19",
        "outputId": "7c9d12e8-afc1-4e3e-8b7e-db787531efd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dropout_p = 0: tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
            "dropout_p = 0.5: tensor([[ 0.,  0.,  4.,  0.,  0., 10., 12., 14.],\n",
            "        [16., 18.,  0., 22., 24.,  0.,  0., 30.]])\n",
            "dropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutMLPScratch(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.LazyLinear(num_hiddens_1)\n",
        "        self.lin2 = nn.LazyLinear(num_hiddens_2)\n",
        "        self.lin3 = nn.LazyLinear(num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_1 = dropout_1\n",
        "        self.dropout_2 = dropout_2\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
        "\n",
        "        # This ensures that your custom dropout_layer is only applied when self.training is True (i.e., when you've called model.train()). When you call model.eval(), these if conditions will be False, and the dropout layers will be bypassed, which is the correct behavior for inference.\n",
        "\n",
        "        if self.training:\n",
        "            H1 = dropout_layer(H1, self.dropout_1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training:\n",
        "            H2 = dropout_layer(H2, self.dropout_2)\n",
        "        return self.lin3(H2)"
      ],
      "metadata": {
        "id": "cZC82zaeXJAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
        "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}"
      ],
      "metadata": {
        "id": "-Tdu0enHaBiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "lr = 0.1\n",
        "max_epochs = 3\n",
        "\n",
        "# Data loading\n",
        "transformer = transforms.ToTensor()\n",
        "mnist_train = datasets.FashionMNIST(root=\"../data\", train=True, transform=transformer, download=True)\n",
        "mnist_val = datasets.FashionMNIST(root=\"../data\", train=False, transform=transformer, download=True)\n",
        "\n",
        "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4)\n",
        "val_iter = data.DataLoader(mnist_val, batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSvkOL-XlRQ1",
        "outputId": "c76cbfa0-f890-422c-d9e4-877799e5538f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DropoutMLPScratch(**hparams)"
      ],
      "metadata": {
        "id": "cK0k3UqUaEJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "vNaHRuwik6Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(max_epochs):\n",
        "  model.train()\n",
        "  train_loss_sum, train_accuracy_sum, n = 0.0, 0.0, 0\n",
        "  for x, y in train_iter:\n",
        "    y_pred = model(x)\n",
        "    l = loss_fn(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    train_loss_sum += l\n",
        "    predicted_labels = torch.argmax(y_pred, dim=1)\n",
        "    train_accuracy_sum += (predicted_labels == y).float().sum()\n",
        "    n += y.numel()\n",
        "\n",
        "  model.eval()\n",
        "  test_accuracy_sum, test_n = 0.0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_iter:\n",
        "      y_pred = model(x)\n",
        "      predicted_labels = torch.argmax(y_pred, dim=1)\n",
        "      test_accuracy_sum += (predicted_labels == y).float().sum()\n",
        "      test_n += y.numel()\n",
        "  test_accuracy = test_accuracy_sum / test_n\n",
        "  print(f'Epoch {epoch + 1}, Loss: {train_loss_sum / n:.4f}, Train Accuracy: {train_accuracy_sum / n:.4f}, Validation Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI-PwTJ4ljRz",
        "outputId": "92eb20f0-369d-4ec4-a985-c80df05fb406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0030, Train Accuracy: 0.7236, Validation Accuracy: 0.8270\n",
            "Epoch 2, Loss: 0.0019, Train Accuracy: 0.8285, Validation Accuracy: 0.8462\n",
            "Epoch 3, Loss: 0.0017, Train Accuracy: 0.8417, Validation Accuracy: 0.8496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutMLPScratch(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(784, num_hiddens_1)\n",
        "        self.lin2 = nn.Linear(num_hiddens_1, num_hiddens_2)\n",
        "        self.lin3 = nn.Linear(num_hiddens_2, num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_1 = dropout_1\n",
        "        self.dropout_2 = dropout_2\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
        "        if self.training:\n",
        "            H1 = dropout_layer(H1, self.dropout_1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training:\n",
        "            H2 = dropout_layer(H2, self.dropout_2)\n",
        "        return self.lin3(H2)"
      ],
      "metadata": {
        "id": "1Hkj8nHYftLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
        "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}"
      ],
      "metadata": {
        "id": "H0dNXDyyf5ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_linear_scratch = DropoutMLPScratch(**hparams)"
      ],
      "metadata": {
        "id": "NgpvisM0f9wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Sequential"
      ],
      "metadata": {
        "id": "x3OSELmFohb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutMLP(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(),\n",
        "            nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(),\n",
        "            nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))\n",
        "    def forward(self, X):\n",
        "      return self.net(X)"
      ],
      "metadata": {
        "id": "qs8Q6fEch-zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sequential = DropoutMLP(**hparams)"
      ],
      "metadata": {
        "id": "8x4ju_8QiIUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_sequential.parameters(), lr = 0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "SpMc0T31m9c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(max_epochs):\n",
        "  model.train()\n",
        "  train_loss_sum, train_accuracy_sum, n = 0.0, 0.0, 0\n",
        "  for x, y in train_iter:\n",
        "    y_pred = model_sequential(x)\n",
        "    l = loss_fn(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    train_loss_sum += l\n",
        "    predicted_labels = torch.argmax(y_pred, dim=1)\n",
        "    train_accuracy_sum += (predicted_labels == y).float().sum()\n",
        "    n += y.numel()\n",
        "\n",
        "  model.eval()\n",
        "  test_accuracy_sum, test_n = 0.0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_iter:\n",
        "      y_pred = model_sequential(x)\n",
        "      predicted_labels = torch.argmax(y_pred, dim=1)\n",
        "      test_accuracy_sum += (predicted_labels == y).float().sum()\n",
        "      test_n += y.numel()\n",
        "  test_accuracy = test_accuracy_sum / test_n\n",
        "  print(f'Epoch {epoch + 1}, Loss: {train_loss_sum / n:.4f}, Train Accuracy: {train_accuracy_sum / n:.4f}, Validation Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "KwQcY5S1nHfo",
        "outputId": "b15c4289-997e-445f-a768-2b49296b4deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0029, Train Accuracy: 0.7272, Validation Accuracy: 0.8007\n",
            "Epoch 2, Loss: 0.0019, Train Accuracy: 0.8278, Validation Accuracy: 0.8248\n",
            "Epoch 3, Loss: 0.0017, Train Accuracy: 0.8438, Validation Accuracy: 0.8309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutMLP_linear(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(), nn.Linear(784, num_hiddens_1), nn.ReLU(),\n",
        "            nn.Dropout(dropout_1), nn.Linear(num_hiddens_1, num_hiddens_2), nn.ReLU(),\n",
        "            nn.Dropout(dropout_2), nn.Linear(num_hiddens_2, num_outputs))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)"
      ],
      "metadata": {
        "id": "UgulQDN5iYif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_linear_sequential = DropoutMLP_linear(**hparams)"
      ],
      "metadata": {
        "id": "7HBdiiCwjUO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With nn.Parameter"
      ],
      "metadata": {
        "id": "l6OPR2rBhUiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutMLPScratch(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super().__init__()\n",
        "        input_dim = 784\n",
        "        self.W1 = nn.Parameter(torch.randn(input_dim, num_hiddens_1) * 0.01)\n",
        "        self.b1 = nn.Parameter(torch.zeros(num_hiddens_1))\n",
        "\n",
        "        self.W2 = nn.Parameter(torch.randn(num_hiddens_1, num_hiddens_2) * 0.01)\n",
        "        self.b2 = nn.Parameter(torch.zeros(num_hiddens_2))\n",
        "\n",
        "        self.W3 = nn.Parameter(torch.randn(num_hiddens_2, num_outputs) * 0.01)\n",
        "        self.b3 = nn.Parameter(torch.zeros(num_outputs))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X.reshape((X.shape[0], -1)) # Flatten the input\n",
        "        H1 = self.relu(torch.matmul(X, self.W1) + self.b1)\n",
        "        if self.training:\n",
        "            H1 = dropout_layer(H1, self.dropout_1)\n",
        "        H2 = self.relu(torch.matmul(H1, self.W2) + self.b2)\n",
        "        if self.training:\n",
        "            H2 = dropout_layer(H2, self.dropout_2)\n",
        "        return torch.matmul(H2, self.W3) + self.b3"
      ],
      "metadata": {
        "id": "2gUWZKP_hUOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}