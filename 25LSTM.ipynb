{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/25LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "u8i7qfOaU6k8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "class TimeMachine():\n",
        "    def __init__(self, batch_size = 2, num_steps = 10, num_train=10000, num_val=5000):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_steps = num_steps\n",
        "        self.num_train = num_train\n",
        "        self.num_val = num_val\n",
        "        corpus, self.vocab = self.build(self._download())\n",
        "        array = torch.tensor([corpus[i:i+num_steps+1]\n",
        "                            for i in range(len(corpus)-num_steps)])\n",
        "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
        "\n",
        "    def _download(self):\n",
        "        url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.text\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return list(text)\n",
        "\n",
        "    def build(self, raw_text, vocab=None):\n",
        "        tokens = self._tokenize(self._preprocess(raw_text))\n",
        "        if vocab is None: vocab = Vocab(tokens)\n",
        "        corpus = [vocab[token] for token in tokens]\n",
        "        return corpus, vocab\n",
        "\n",
        "    def get_tensorloader(self, data_arrays, train, i):\n",
        "        # This is a placeholder, actual implementation might vary based on inheritance.\n",
        "        features = data_arrays[0][i]\n",
        "        labels = data_arrays[1][i]\n",
        "        dataset = TensorDataset(features, labels)\n",
        "        dataloader = DataLoader(dataset, self.batch_size, shuffle=train)\n",
        "        return dataloader\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        idx = slice(0, self.num_train) if train else slice(\n",
        "            self.num_train, self.num_train + self.num_val)\n",
        "        return self.get_tensorloader([self.X, self.Y], train, idx)"
      ],
      "metadata": {
        "id": "rg5IedAUWkBo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set([''] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['']"
      ],
      "metadata": {
        "id": "jYtXZhlZWsj6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLMScratch(nn.Module):\n",
        "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
        "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.rnn = rnn # Store the RNN instance\n",
        "        self.vocab_size = vocab_size\n",
        "        self.loss = nn.CrossEntropyLoss() # Initialize the loss function\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        self.W_hq = nn.Parameter(\n",
        "            torch.randn(\n",
        "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
        "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        # self.plot('ppl', torch.exp(l), train=True) # Removed as self.plot is undefined\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        # self.plot('ppl', torch.exp(l), train=False) # Removed as self.plot is undefined\n",
        "\n",
        "    def one_hot(self, X):\n",
        "        # Output shape: (num_steps, batch_size, vocab_size)\n",
        "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "\n",
        "    def output_layer(self, rnn_outputs):\n",
        "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
        "        return torch.stack(outputs, 1)\n",
        "\n",
        "    def forward(self, X, state=None):\n",
        "        embs = self.one_hot(X)\n",
        "        rnn_outputs, _ = self.rnn(embs, state)\n",
        "        return self.output_layer(rnn_outputs)\n",
        "\n",
        "    def predict(self, prefix, num_preds, vocab, device=None):\n",
        "        state, outputs = None, [vocab[prefix[0]]]\n",
        "        for i in range(len(prefix) + num_preds - 1):\n",
        "            X = torch.tensor([[outputs[-1]]], device=device)\n",
        "            embs = self.one_hot(X)\n",
        "            rnn_outputs, state = self.rnn(embs, state)\n",
        "            if i < len(prefix) - 1:  # Warm-up period\n",
        "                outputs.append(vocab[prefix[i + 1]])\n",
        "            else:  # Predict num_preds steps\n",
        "                Y = self.output_layer(rnn_outputs)\n",
        "                outputs.append(int(Y.argmax(axis=2).reshape(1)))\n",
        "        return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "bFvev1GDX0Ch"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, lambda is used to create small, anonymous functions. They are typically defined in a single line and can take any number of arguments but can only have one expression.\n",
        "\n",
        "The *shape part is called arbitrary argument unpacking. When you define a function (or a lambda function) with *args (where args can be any name, like shape in your case), it means the function can accept a variable number of positional arguments. These arguments are then collected into a tuple inside the function.\n",
        "\n",
        "So, lambda *shape: means you're defining an anonymous function that can accept any number of positional arguments, and these arguments will be bundled together into a tuple named shape within the function's body."
      ],
      "metadata": {
        "id": "v1Dl_jRJiCGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use an example of how lambda *shape works within your init_weight function. Imagine you are initializing the W_xi weight matrix in your LSTMScratch class. This matrix connects the num_inputs to num_hiddens.\n",
        "\n",
        "When you define the W_xi parameter, you might call init_weight(num_inputs, num_hiddens). Let's say num_inputs is 128 and num_hiddens is 64.\n",
        "\n",
        "Calling the lambda: You would essentially call init_weight(128, 64).\n",
        "*shape in action: Inside the lambda *shape: function, the *shape collects these two separate arguments (128 and 64) into a single tuple. So, the variable shape inside the lambda becomes (128, 64).\n",
        "Further usage: This (128, 64) tuple is then passed to torch.randn(*shape), which unpacks the tuple back into individual arguments, effectively becoming torch.randn(128, 64), creating a 128x64 random tensor."
      ],
      "metadata": {
        "id": "VRXEXvk7iZ6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------"
      ],
      "metadata": {
        "id": "hMffJSAzj50e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explain the line triple = lambda: (init_weight(num_inputs, num_hiddens), init_weight(num_hiddens, num_hiddens), nn.Parameter(torch.zeros(num_hiddens))) with an example.\n",
        "\n",
        "This triple lambda function is a convenient way to create three related parameters for an LSTM gate (like the input gate, forget gate, or output gate) in one go. It's a function that takes no arguments and, when called, returns a tuple containing three nn.Parameter objects.\n",
        "\n",
        "Let's assume:\n",
        "\n",
        "num_inputs = 128 (e.g., the size of your one-hot encoded input character)\n",
        "num_hiddens = 64 (e.g., the dimension of your LSTM's hidden state)\n",
        "sigma = 0.01\n",
        "When you call triple(), here's what happens:\n",
        "\n",
        "init_weight(num_inputs, num_hiddens): This first call uses the init_weight lambda we discussed earlier. It will create an nn.Parameter which is a 128x64 matrix of small, randomly initialized numbers (multiplied by sigma). This represents the weights (W_x*) that connect the input X to the specific gate.\n",
        "\n",
        "init_weight(num_hiddens, num_hiddens): The second call to init_weight creates another nn.Parameter, this time a 64x64 matrix of small, random numbers. This represents the recurrent weights (W_h*) that connect the previous hidden state H to the same gate.\n",
        "\n",
        "nn.Parameter(torch.zeros(num_hiddens)): This third part creates an nn.Parameter which is a 64-element vector filled with zeros. This represents the bias vector (b_*) for the gate.\n",
        "\n",
        "So, when triple() is executed, it returns a tuple like this: (random_128x64_matrix, random_64x64_matrix, zero_64_vector).\n",
        "\n",
        "This tuple is then unpacked into self.W_xi, self.W_hi, self.b_i (for the input gate), self.W_xf, self.W_hf, self.b_f (for the forget gate), and so on. This neatly packages the initialization of all the weights and biases for each LSTM gate."
      ],
      "metadata": {
        "id": "bbGd-ygBj68Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------"
      ],
      "metadata": {
        "id": "VgET6P0opT_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial State Handling (if H_C is None:):\n",
        "\n",
        "If no previous hidden state (H) or cell state (C) is provided, the function initializes them to zero tensors. These are typically (batch_size, num_hiddens) in shape. This is crucial for the very first step of processing a sequence.\n",
        "If H_C is provided, it means this is not the first step of the sequence, and the model continues from where it left off.\n",
        "Loop over Inputs (for X in inputs:):\n",
        "\n",
        "The inputs typically represents a sequence, where each X is a single time step's input (e.g., a one-hot encoded character). The model processes each X one by one.\n",
        "LSTM Gate Calculations (Inside the loop): This is where the magic of LSTM happens, involving four main components for each time step:\n",
        "\n",
        "Input Gate (I): I = torch.sigmoid(torch.matmul(X, self.W_xi) + torch.matmul(H, self.W_hi) + self.b_i)\n",
        "\n",
        "This gate decides how much of the new information from the current input (X) and previous hidden state (H) should be let into the cell state. It uses a sigmoid activation, producing values between 0 and 1.\n",
        "self.W_xi and self.W_hi are the weight matrices for the input and recurrent connections, and self.b_i is the bias.\n",
        "Forget Gate (F): F = torch.sigmoid(torch.matmul(X, self.W_xf) + torch.matmul(H, self.W_hf) + self.b_f)\n",
        "\n",
        "This gate decides what information from the previous cell state (C) should be forgotten. Also uses a sigmoid activation.\n",
        "Output Gate (O): O = torch.sigmoid(torch.matmul(X, self.W_xo) + torch.matmul(H, self.W_ho) + self.b_o)\n",
        "\n",
        "This gate decides what part of the current cell state (C) should be exposed as the new hidden state (H). Uses a sigmoid activation.\n",
        "Candidate Cell State (C_tilde): C_tilde = torch.tanh(torch.matmul(X, self.W_xc) + torch.matmul(H, self.W_hc) + self.b_c)\n",
        "\n",
        "This generates a candidate for the new cell state. It uses a tanh activation, producing values between -1 and 1.\n",
        "Cell State Update (C = F * C + I * C_tilde):\n",
        "\n",
        "This is the core of LSTM's ability to remember long-term dependencies. The previous cell state (C) is scaled by the forget gate (F), effectively 'forgetting' old information. The candidate cell state (C_tilde) is scaled by the input gate (I), effectively 'adding' new relevant information. These two terms are summed to produce the new cell state.\n",
        "Hidden State Update (H = O * torch.tanh(C)):\n",
        "\n",
        "The new hidden state (H) is computed by taking the tanh of the new cell state (C) and scaling it by the output gate (O). This allows the model to selectively output information from its cell state.\n",
        "Storing Outputs (outputs.append(H)):\n",
        "\n",
        "The new hidden state H for the current time step is added to a list. These hidden states often serve as the output of the RNN layer for downstream tasks (like feeding into a classification or prediction layer).\n",
        "Return Value (return outputs, (H, C)):\n",
        "\n",
        "The function returns the list of hidden states for all time steps processed and the final hidden and cell states (H, C) for the last time step, which can be passed as the initial state for the next sequence."
      ],
      "metadata": {
        "id": "KFU_s8BBpVD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMScratch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.sigma = sigma # Store sigma as an instance attribute\n",
        "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
        "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
        "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
        "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
        "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        if H_C is None:\n",
        "            # Initial state with shape: (batch_size, num_hiddens)\n",
        "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                          device=inputs.device)\n",
        "        else:\n",
        "            H, C = H_C\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
        "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
        "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
        "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
        "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
        "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
        "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
        "                               torch.matmul(H, self.W_hc) + self.b_c)\n",
        "            C = F * C + I * C_tilde\n",
        "            H = O * torch.tanh(C)\n",
        "            outputs.append(H)\n",
        "        return outputs, (H, C)"
      ],
      "metadata": {
        "id": "F-a3rqH4XE3F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = TimeMachine(batch_size=1024, num_steps=32)\n",
        "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
        "model = RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=0.001)"
      ],
      "metadata": {
        "id": "VQjZNvPtX4do"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdd2df30",
        "outputId": "a418aaa0-0e7d-4302-9dff-ac01d7f2cdb6"
      },
      "source": [
        "prefix = 'it has a time'\n",
        "num_preds = 50\n",
        "predicted_text = model.predict(prefix, num_preds, data.vocab, device=None)\n",
        "print(predicted_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it has a timezjjmjmyy yy yy y yy yy y yy yy y yy yy y yy yy y y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32012117"
      },
      "source": [
        "def train(model, data, num_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # Check if a CUDA-enabled GPU is available, otherwise use the CPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl_total, num_tokens = 0, 0\n",
        "        model.train() # Set the model to training mode\n",
        "        for X, Y in data.get_dataloader(train=True):\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            # Ensure Y has the correct shape for CrossEntropyLoss (batch_size * sequence_length)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            Y = Y.reshape(-1)\n",
        "            loss = model.loss(output, Y)\n",
        "            loss.backward()\n",
        "            # Clip gradients to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ppl_total += torch.exp(loss) * Y.numel()\n",
        "                num_tokens += Y.numel()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Perplexity: {ppl_total / num_tokens:.2f}')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bf19357"
      },
      "source": [
        "lr = 0.001"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b882c0",
        "outputId": "7bcdf58c-618f-4405-a7db-560c9b549143"
      },
      "source": [
        "num_epochs = 10\n",
        "train(model, data, num_epochs, lr)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Perplexity: 27.88\n",
            "Epoch 2, Perplexity: 27.53\n",
            "Epoch 3, Perplexity: 26.81\n",
            "Epoch 4, Perplexity: 24.47\n",
            "Epoch 5, Perplexity: 20.24\n",
            "Epoch 6, Perplexity: 18.79\n",
            "Epoch 7, Perplexity: 18.25\n",
            "Epoch 8, Perplexity: 18.03\n",
            "Epoch 9, Perplexity: 17.88\n",
            "Epoch 10, Perplexity: 17.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a204cd9",
        "outputId": "b44bca95-ace5-4e01-ab74-02b804482884"
      },
      "source": [
        "prefix = 'it has a time'\n",
        "num_preds = 50\n",
        "device = next(model.parameters()).device # Get the device where the model is located\n",
        "predicted_text_new = model.predict(prefix, num_preds, data.vocab, device=device)\n",
        "print(predicted_text_new)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it has a time                                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concise Implementation"
      ],
      "metadata": {
        "id": "1865DKB0c7av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.sigma = sigma # Store sigma as an instance attribute\n",
        "        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        return self.rnn(inputs, H_C)"
      ],
      "metadata": {
        "id": "He4TZNCzc9XZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32, sigma=0.01)\n",
        "model = RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=0.001)"
      ],
      "metadata": {
        "id": "1cN-LstvdLAH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a388a688",
        "outputId": "b2624b11-21c8-47f5-885b-0f2c9ce2311b"
      },
      "source": [
        "num_epochs = 100\n",
        "lr = 0.001\n",
        "train(model, data, num_epochs, lr)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Perplexity: 27.70\n",
            "Epoch 2, Perplexity: 26.75\n",
            "Epoch 3, Perplexity: 24.72\n",
            "Epoch 4, Perplexity: 21.20\n",
            "Epoch 5, Perplexity: 19.07\n",
            "Epoch 6, Perplexity: 18.19\n",
            "Epoch 7, Perplexity: 17.88\n",
            "Epoch 8, Perplexity: 17.71\n",
            "Epoch 9, Perplexity: 17.61\n",
            "Epoch 10, Perplexity: 17.54\n",
            "Epoch 11, Perplexity: 17.49\n",
            "Epoch 12, Perplexity: 17.45\n",
            "Epoch 13, Perplexity: 17.41\n",
            "Epoch 14, Perplexity: 17.38\n",
            "Epoch 15, Perplexity: 17.34\n",
            "Epoch 16, Perplexity: 17.31\n",
            "Epoch 17, Perplexity: 17.27\n",
            "Epoch 18, Perplexity: 17.22\n",
            "Epoch 19, Perplexity: 17.17\n",
            "Epoch 20, Perplexity: 17.12\n",
            "Epoch 21, Perplexity: 17.06\n",
            "Epoch 22, Perplexity: 16.99\n",
            "Epoch 23, Perplexity: 16.91\n",
            "Epoch 24, Perplexity: 16.82\n",
            "Epoch 25, Perplexity: 16.73\n",
            "Epoch 26, Perplexity: 16.63\n",
            "Epoch 27, Perplexity: 16.52\n",
            "Epoch 28, Perplexity: 16.39\n",
            "Epoch 29, Perplexity: 16.26\n",
            "Epoch 30, Perplexity: 16.12\n",
            "Epoch 31, Perplexity: 15.97\n",
            "Epoch 32, Perplexity: 15.82\n",
            "Epoch 33, Perplexity: 15.65\n",
            "Epoch 34, Perplexity: 15.47\n",
            "Epoch 35, Perplexity: 15.29\n",
            "Epoch 36, Perplexity: 15.09\n",
            "Epoch 37, Perplexity: 14.88\n",
            "Epoch 38, Perplexity: 14.66\n",
            "Epoch 39, Perplexity: 14.44\n",
            "Epoch 40, Perplexity: 14.23\n",
            "Epoch 41, Perplexity: 14.01\n",
            "Epoch 42, Perplexity: 13.81\n",
            "Epoch 43, Perplexity: 13.61\n",
            "Epoch 44, Perplexity: 13.41\n",
            "Epoch 45, Perplexity: 13.22\n",
            "Epoch 46, Perplexity: 13.03\n",
            "Epoch 47, Perplexity: 12.84\n",
            "Epoch 48, Perplexity: 12.66\n",
            "Epoch 49, Perplexity: 12.48\n",
            "Epoch 50, Perplexity: 12.31\n",
            "Epoch 51, Perplexity: 12.14\n",
            "Epoch 52, Perplexity: 11.97\n",
            "Epoch 53, Perplexity: 11.80\n",
            "Epoch 54, Perplexity: 11.63\n",
            "Epoch 55, Perplexity: 11.47\n",
            "Epoch 56, Perplexity: 11.31\n",
            "Epoch 57, Perplexity: 11.16\n",
            "Epoch 58, Perplexity: 11.02\n",
            "Epoch 59, Perplexity: 10.89\n",
            "Epoch 60, Perplexity: 10.77\n",
            "Epoch 61, Perplexity: 10.65\n",
            "Epoch 62, Perplexity: 10.54\n",
            "Epoch 63, Perplexity: 10.44\n",
            "Epoch 64, Perplexity: 10.34\n",
            "Epoch 65, Perplexity: 10.25\n",
            "Epoch 66, Perplexity: 10.17\n",
            "Epoch 67, Perplexity: 10.08\n",
            "Epoch 68, Perplexity: 10.01\n",
            "Epoch 69, Perplexity: 9.93\n",
            "Epoch 70, Perplexity: 9.86\n",
            "Epoch 71, Perplexity: 9.79\n",
            "Epoch 72, Perplexity: 9.73\n",
            "Epoch 73, Perplexity: 9.67\n",
            "Epoch 74, Perplexity: 9.61\n",
            "Epoch 75, Perplexity: 9.55\n",
            "Epoch 76, Perplexity: 9.49\n",
            "Epoch 77, Perplexity: 9.44\n",
            "Epoch 78, Perplexity: 9.39\n",
            "Epoch 79, Perplexity: 9.33\n",
            "Epoch 80, Perplexity: 9.29\n",
            "Epoch 81, Perplexity: 9.24\n",
            "Epoch 82, Perplexity: 9.19\n",
            "Epoch 83, Perplexity: 9.15\n",
            "Epoch 84, Perplexity: 9.10\n",
            "Epoch 85, Perplexity: 9.06\n",
            "Epoch 86, Perplexity: 9.02\n",
            "Epoch 87, Perplexity: 8.98\n",
            "Epoch 88, Perplexity: 8.94\n",
            "Epoch 89, Perplexity: 8.90\n",
            "Epoch 90, Perplexity: 8.86\n",
            "Epoch 91, Perplexity: 8.82\n",
            "Epoch 92, Perplexity: 8.79\n",
            "Epoch 93, Perplexity: 8.75\n",
            "Epoch 94, Perplexity: 8.72\n",
            "Epoch 95, Perplexity: 8.68\n",
            "Epoch 96, Perplexity: 8.65\n",
            "Epoch 97, Perplexity: 8.62\n",
            "Epoch 98, Perplexity: 8.58\n",
            "Epoch 99, Perplexity: 8.55\n",
            "Epoch 100, Perplexity: 8.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = next(model.parameters()).device # Get the device where the model is located\n",
        "model.predict('it has', 20, data.vocab, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ENvO44XHd3PA",
        "outputId": "33f7edb0-b990-4ed1-decd-906c7bf77b33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it has the the the the the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}