{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/28BidirectionalRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "VcMw_H8_HR6F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "class TimeMachine():\n",
        "    def __init__(self, batch_size = 2, num_steps = 10, num_train=10000, num_val=5000):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_steps = num_steps\n",
        "        self.num_train = num_train\n",
        "        self.num_val = num_val\n",
        "        corpus, self.vocab = self.build(self._download())\n",
        "        array = torch.tensor([corpus[i:i+num_steps+1]\n",
        "                            for i in range(len(corpus)-num_steps)])\n",
        "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
        "\n",
        "    def _download(self):\n",
        "        url = 'http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        return response.text\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return list(text)\n",
        "\n",
        "    def build(self, raw_text, vocab=None):\n",
        "        tokens = self._tokenize(self._preprocess(raw_text))\n",
        "        if vocab is None: vocab = Vocab(tokens)\n",
        "        corpus = [vocab[token] for token in tokens]\n",
        "        return corpus, vocab\n",
        "\n",
        "    def get_tensorloader(self, data_arrays, train, i):\n",
        "        # This is a placeholder, actual implementation might vary based on inheritance.\n",
        "        features = data_arrays[0][i]\n",
        "        labels = data_arrays[1][i]\n",
        "        dataset = TensorDataset(features, labels)\n",
        "        dataloader = DataLoader(dataset, self.batch_size, shuffle=train)\n",
        "        return dataloader\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        idx = slice(0, self.num_train) if train else slice(\n",
        "            self.num_train, self.num_train + self.num_val)\n",
        "        return self.get_tensorloader([self.X, self.Y], train, idx)"
      ],
      "metadata": {
        "id": "emny1s9qHXln"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set([''] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['']\n"
      ],
      "metadata": {
        "id": "RTYYuA3KHgMy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLMScratch(nn.Module):\n",
        "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
        "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.rnn = rnn # Store the RNN instance\n",
        "        self.vocab_size = vocab_size\n",
        "        self.loss = nn.CrossEntropyLoss() # Initialize the loss function\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        self.W_hq = nn.Parameter(\n",
        "            torch.randn(\n",
        "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
        "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        # self.plot('ppl', torch.exp(l), train=True) # Removed as self.plot is undefined\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        # self.plot('ppl', torch.exp(l), train=False) # Removed as self.plot is undefined\n",
        "\n",
        "    def one_hot(self, X):\n",
        "        # Output shape: (num_steps, batch_size, vocab_size)\n",
        "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "\n",
        "    def output_layer(self, rnn_outputs):\n",
        "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
        "        return torch.stack(outputs, 1)\n",
        "\n",
        "    def forward(self, X, state=None):\n",
        "        embs = self.one_hot(X)\n",
        "        rnn_outputs, _ = self.rnn(embs, state)\n",
        "        return self.output_layer(rnn_outputs)\n",
        "\n",
        "    def predict(self, prefix, num_preds, vocab, device=None):\n",
        "        state, outputs = None, [vocab[prefix[0]]]\n",
        "        for i in range(len(prefix) + num_preds - 1):\n",
        "            X = torch.tensor([[outputs[-1]]], device=device)\n",
        "            embs = self.one_hot(X)\n",
        "            rnn_outputs, state = self.rnn(embs, state)\n",
        "            if i < len(prefix) - 1:  # Warm-up period\n",
        "                outputs.append(vocab[prefix[i + 1]])\n",
        "            else:  # Predict num_preds steps\n",
        "                Y = self.output_layer(rnn_outputs)\n",
        "                outputs.append(int(Y.argmax(axis=2).reshape(1)))\n",
        "        return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "m9UrwWssHx4i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNScratch(nn.Module):\n",
        "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.sigma = sigma # Store sigma as an instance attribute\n",
        "        self.W_xh = nn.Parameter(\n",
        "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
        "        self.W_hh = nn.Parameter(\n",
        "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
        "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n",
        "\n",
        "    def forward(self, inputs, state=None):\n",
        "        if state is None:\n",
        "            # Initial state with shape: (batch_size, num_hiddens)\n",
        "            state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                              device=inputs.device)\n",
        "        else:\n",
        "            state, = state\n",
        "        outputs = []\n",
        "        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
        "            state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
        "                             torch.matmul(state, self.W_hh) + self.b_h)\n",
        "            outputs.append(state)\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "FqvUvkekIsmf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down torch.cat((f, b), -1) with an example.\n",
        "\n",
        "torch.cat() is a PyTorch function used to concatenate (join) a sequence of tensors along a given dimension. The (f, b) part means you are concatenating two tensors, f and b.\n",
        "\n",
        "The -1 specifies the dimension along which to concatenate. In PyTorch, -1 refers to the last dimension of the tensor. For example:\n",
        "\n",
        "If you have a 1D tensor (a vector), -1 is the 0th dimension.\n",
        "If you have a 2D tensor (a matrix), -1 is the 1st dimension (columns).\n",
        "If you have a 3D tensor, -1 is the 2nd dimension.\n",
        "In the context of the BiRNNScratch class, f represents the forward hidden state and b represents the backward hidden state. Both f and b would typically have the shape (batch_size, num_hiddens) for a single timestep.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's assume:\n",
        "\n",
        "f is a tensor representing the forward hidden state for a batch of 2, with 64 hidden units: f = torch.randn(2, 64) (shape: [2, 64])\n",
        "b is a tensor representing the backward hidden state for the same batch and number of hidden units: b = torch.randn(2, 64) (shape: [2, 64])\n",
        "When you call torch.cat((f, b), -1):\n",
        "\n",
        "Input Tensors: f and b both have shape [2, 64]. The dimensions are 0 (batch size) and 1 (hidden units).\n",
        "Concatenation Dimension: -1 refers to the last dimension, which is dimension 1 (the hidden units dimension).\n",
        "Operation: PyTorch will join f and b along dimension 1. The elements of b will be appended to the elements of f along this dimension.\n",
        "Resulting Tensor:\n",
        "\n",
        "The output tensor will have a shape of [2, 128]. The batch size (dimension 0) remains the same (2), but the last dimension (dimension 1) becomes the sum of the corresponding dimensions of f and b (64 + 64 = 128).\n",
        "\n",
        "Essentially, it combines the forward and backward hidden states for each item in the batch, creating a richer representation that captures information from both directions of the sequence."
      ],
      "metadata": {
        "id": "dlHK90vH11O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiRNNScratch(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens # Initialize num_hiddens\n",
        "        self.sigma = sigma # Store sigma as an instance attribute\n",
        "        self.f_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
        "        self.b_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
        "        self.num_hiddens *= 2  # The output dimension will be doubled\n",
        "\n",
        "    def forward(self, inputs, Hs=None):\n",
        "        f_H, b_H = Hs if Hs is not None else (None, None)\n",
        "        f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
        "        b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
        "        outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
        "            f_outputs, reversed(b_outputs))]\n",
        "        return outputs, (f_H, b_H)"
      ],
      "metadata": {
        "id": "_1l7YMWHuNx4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = TimeMachine(batch_size=1024, num_steps=32)\n",
        "rnn_block = BiRNNScratch(num_inputs=len(data.vocab),\n",
        "                              num_hiddens=128)\n",
        "model = RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=0.001)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttw64C6MJZhO",
        "outputId": "7ee4e57d-270e-4975-b3bc-e093a768489c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLMScratch(\n",
              "  (rnn): BiRNNScratch(\n",
              "    (f_rnn): RNNScratch()\n",
              "    (b_rnn): RNNScratch()\n",
              "  )\n",
              "  (loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, num_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # Check if a CUDA-enabled GPU is available, otherwise use the CPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl_total, num_tokens = 0, 0\n",
        "        model.train() # Set the model to training mode\n",
        "        for X, Y in data.get_dataloader(train=True):\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            # Ensure Y has the correct shape for CrossEntropyLoss (batch_size * sequence_length)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            Y = Y.reshape(-1)\n",
        "            loss = model.loss(output, Y)\n",
        "            loss.backward()\n",
        "            # Clip gradients to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ppl_total += torch.exp(loss) * Y.numel()\n",
        "                num_tokens += Y.numel()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Perplexity: {ppl_total / num_tokens:.2f}')"
      ],
      "metadata": {
        "id": "RNAJ1yZYJ0TE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "num_epochs = 300\n",
        "train(model, data, num_epochs, lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXLNItpvJ5FR",
        "outputId": "12b3e478-4367-454b-8b9c-8833706d9b01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Perplexity: 27.39\n",
            "Epoch 2, Perplexity: 20.47\n",
            "Epoch 3, Perplexity: 17.24\n",
            "Epoch 4, Perplexity: 16.13\n",
            "Epoch 5, Perplexity: 14.31\n",
            "Epoch 6, Perplexity: 11.67\n",
            "Epoch 7, Perplexity: 8.93\n",
            "Epoch 8, Perplexity: 6.75\n",
            "Epoch 9, Perplexity: 5.15\n",
            "Epoch 10, Perplexity: 3.99\n",
            "Epoch 11, Perplexity: 3.15\n",
            "Epoch 12, Perplexity: 2.57\n",
            "Epoch 13, Perplexity: 2.15\n",
            "Epoch 14, Perplexity: 1.85\n",
            "Epoch 15, Perplexity: 1.64\n",
            "Epoch 16, Perplexity: 1.49\n",
            "Epoch 17, Perplexity: 1.38\n",
            "Epoch 18, Perplexity: 1.31\n",
            "Epoch 19, Perplexity: 1.25\n",
            "Epoch 20, Perplexity: 1.21\n",
            "Epoch 21, Perplexity: 1.18\n",
            "Epoch 22, Perplexity: 1.16\n",
            "Epoch 23, Perplexity: 1.15\n",
            "Epoch 24, Perplexity: 1.13\n",
            "Epoch 25, Perplexity: 1.13\n",
            "Epoch 26, Perplexity: 1.12\n",
            "Epoch 27, Perplexity: 1.11\n",
            "Epoch 28, Perplexity: 1.11\n",
            "Epoch 29, Perplexity: 1.11\n",
            "Epoch 30, Perplexity: 1.10\n",
            "Epoch 31, Perplexity: 1.10\n",
            "Epoch 32, Perplexity: 1.10\n",
            "Epoch 33, Perplexity: 1.10\n",
            "Epoch 34, Perplexity: 1.09\n",
            "Epoch 35, Perplexity: 1.09\n",
            "Epoch 36, Perplexity: 1.09\n",
            "Epoch 37, Perplexity: 1.09\n",
            "Epoch 38, Perplexity: 1.09\n",
            "Epoch 39, Perplexity: 1.09\n",
            "Epoch 40, Perplexity: 1.09\n",
            "Epoch 41, Perplexity: 1.09\n",
            "Epoch 42, Perplexity: 1.09\n",
            "Epoch 43, Perplexity: 1.09\n",
            "Epoch 44, Perplexity: 1.09\n",
            "Epoch 45, Perplexity: 1.09\n",
            "Epoch 46, Perplexity: 1.08\n",
            "Epoch 47, Perplexity: 1.08\n",
            "Epoch 48, Perplexity: 1.08\n",
            "Epoch 49, Perplexity: 1.08\n",
            "Epoch 50, Perplexity: 1.08\n",
            "Epoch 51, Perplexity: 1.08\n",
            "Epoch 52, Perplexity: 1.08\n",
            "Epoch 53, Perplexity: 1.08\n",
            "Epoch 54, Perplexity: 1.08\n",
            "Epoch 55, Perplexity: 1.08\n",
            "Epoch 56, Perplexity: 1.08\n",
            "Epoch 57, Perplexity: 1.08\n",
            "Epoch 58, Perplexity: 1.08\n",
            "Epoch 59, Perplexity: 1.08\n",
            "Epoch 60, Perplexity: 1.08\n",
            "Epoch 61, Perplexity: 1.08\n",
            "Epoch 62, Perplexity: 1.08\n",
            "Epoch 63, Perplexity: 1.08\n",
            "Epoch 64, Perplexity: 1.08\n",
            "Epoch 65, Perplexity: 1.08\n",
            "Epoch 66, Perplexity: 1.08\n",
            "Epoch 67, Perplexity: 1.08\n",
            "Epoch 68, Perplexity: 1.08\n",
            "Epoch 69, Perplexity: 1.08\n",
            "Epoch 70, Perplexity: 1.08\n",
            "Epoch 71, Perplexity: 1.08\n",
            "Epoch 72, Perplexity: 1.08\n",
            "Epoch 73, Perplexity: 1.08\n",
            "Epoch 74, Perplexity: 1.08\n",
            "Epoch 75, Perplexity: 1.08\n",
            "Epoch 76, Perplexity: 1.08\n",
            "Epoch 77, Perplexity: 1.08\n",
            "Epoch 78, Perplexity: 1.08\n",
            "Epoch 79, Perplexity: 1.08\n",
            "Epoch 80, Perplexity: 1.08\n",
            "Epoch 81, Perplexity: 1.08\n",
            "Epoch 82, Perplexity: 1.08\n",
            "Epoch 83, Perplexity: 1.08\n",
            "Epoch 84, Perplexity: 1.08\n",
            "Epoch 85, Perplexity: 1.08\n",
            "Epoch 86, Perplexity: 1.08\n",
            "Epoch 87, Perplexity: 1.08\n",
            "Epoch 88, Perplexity: 1.08\n",
            "Epoch 89, Perplexity: 1.08\n",
            "Epoch 90, Perplexity: 1.07\n",
            "Epoch 91, Perplexity: 1.07\n",
            "Epoch 92, Perplexity: 1.07\n",
            "Epoch 93, Perplexity: 1.07\n",
            "Epoch 94, Perplexity: 1.07\n",
            "Epoch 95, Perplexity: 1.07\n",
            "Epoch 96, Perplexity: 1.07\n",
            "Epoch 97, Perplexity: 1.07\n",
            "Epoch 98, Perplexity: 1.07\n",
            "Epoch 99, Perplexity: 1.07\n",
            "Epoch 100, Perplexity: 1.07\n",
            "Epoch 101, Perplexity: 1.07\n",
            "Epoch 102, Perplexity: 1.07\n",
            "Epoch 103, Perplexity: 1.07\n",
            "Epoch 104, Perplexity: 1.07\n",
            "Epoch 105, Perplexity: 1.07\n",
            "Epoch 106, Perplexity: 1.07\n",
            "Epoch 107, Perplexity: 1.07\n",
            "Epoch 108, Perplexity: 1.07\n",
            "Epoch 109, Perplexity: 1.07\n",
            "Epoch 110, Perplexity: 1.07\n",
            "Epoch 111, Perplexity: 1.07\n",
            "Epoch 112, Perplexity: 1.07\n",
            "Epoch 113, Perplexity: 1.07\n",
            "Epoch 114, Perplexity: 1.07\n",
            "Epoch 115, Perplexity: 1.07\n",
            "Epoch 116, Perplexity: 1.07\n",
            "Epoch 117, Perplexity: 1.07\n",
            "Epoch 118, Perplexity: 1.07\n",
            "Epoch 119, Perplexity: 1.07\n",
            "Epoch 120, Perplexity: 1.07\n",
            "Epoch 121, Perplexity: 1.07\n",
            "Epoch 122, Perplexity: 1.07\n",
            "Epoch 123, Perplexity: 1.07\n",
            "Epoch 124, Perplexity: 1.07\n",
            "Epoch 125, Perplexity: 1.07\n",
            "Epoch 126, Perplexity: 1.07\n",
            "Epoch 127, Perplexity: 1.07\n",
            "Epoch 128, Perplexity: 1.07\n",
            "Epoch 129, Perplexity: 1.07\n",
            "Epoch 130, Perplexity: 1.07\n",
            "Epoch 131, Perplexity: 1.07\n",
            "Epoch 132, Perplexity: 1.07\n",
            "Epoch 133, Perplexity: 1.07\n",
            "Epoch 134, Perplexity: 1.07\n",
            "Epoch 135, Perplexity: 1.07\n",
            "Epoch 136, Perplexity: 1.07\n",
            "Epoch 137, Perplexity: 1.07\n",
            "Epoch 138, Perplexity: 1.07\n",
            "Epoch 139, Perplexity: 1.07\n",
            "Epoch 140, Perplexity: 1.07\n",
            "Epoch 141, Perplexity: 1.07\n",
            "Epoch 142, Perplexity: 1.07\n",
            "Epoch 143, Perplexity: 1.07\n",
            "Epoch 144, Perplexity: 1.07\n",
            "Epoch 145, Perplexity: 1.07\n",
            "Epoch 146, Perplexity: 1.07\n",
            "Epoch 147, Perplexity: 1.07\n",
            "Epoch 148, Perplexity: 1.07\n",
            "Epoch 149, Perplexity: 1.07\n",
            "Epoch 150, Perplexity: 1.07\n",
            "Epoch 151, Perplexity: 1.07\n",
            "Epoch 152, Perplexity: 1.07\n",
            "Epoch 153, Perplexity: 1.07\n",
            "Epoch 154, Perplexity: 1.07\n",
            "Epoch 155, Perplexity: 1.07\n",
            "Epoch 156, Perplexity: 1.07\n",
            "Epoch 157, Perplexity: 1.07\n",
            "Epoch 158, Perplexity: 1.07\n",
            "Epoch 159, Perplexity: 1.07\n",
            "Epoch 160, Perplexity: 1.07\n",
            "Epoch 161, Perplexity: 1.07\n",
            "Epoch 162, Perplexity: 1.07\n",
            "Epoch 163, Perplexity: 1.07\n",
            "Epoch 164, Perplexity: 1.07\n",
            "Epoch 165, Perplexity: 1.07\n",
            "Epoch 166, Perplexity: 1.07\n",
            "Epoch 167, Perplexity: 1.07\n",
            "Epoch 168, Perplexity: 1.07\n",
            "Epoch 169, Perplexity: 1.07\n",
            "Epoch 170, Perplexity: 1.07\n",
            "Epoch 171, Perplexity: 1.07\n",
            "Epoch 172, Perplexity: 1.07\n",
            "Epoch 173, Perplexity: 1.07\n",
            "Epoch 174, Perplexity: 1.07\n",
            "Epoch 175, Perplexity: 1.07\n",
            "Epoch 176, Perplexity: 1.07\n",
            "Epoch 177, Perplexity: 1.07\n",
            "Epoch 178, Perplexity: 1.07\n",
            "Epoch 179, Perplexity: 1.07\n",
            "Epoch 180, Perplexity: 1.07\n",
            "Epoch 181, Perplexity: 1.07\n",
            "Epoch 182, Perplexity: 1.07\n",
            "Epoch 183, Perplexity: 1.07\n",
            "Epoch 184, Perplexity: 1.07\n",
            "Epoch 185, Perplexity: 1.07\n",
            "Epoch 186, Perplexity: 1.07\n",
            "Epoch 187, Perplexity: 1.07\n",
            "Epoch 188, Perplexity: 1.07\n",
            "Epoch 189, Perplexity: 1.07\n",
            "Epoch 190, Perplexity: 1.07\n",
            "Epoch 191, Perplexity: 1.07\n",
            "Epoch 192, Perplexity: 1.07\n",
            "Epoch 193, Perplexity: 1.07\n",
            "Epoch 194, Perplexity: 1.07\n",
            "Epoch 195, Perplexity: 1.07\n",
            "Epoch 196, Perplexity: 1.07\n",
            "Epoch 197, Perplexity: 1.07\n",
            "Epoch 198, Perplexity: 1.07\n",
            "Epoch 199, Perplexity: 1.07\n",
            "Epoch 200, Perplexity: 1.07\n",
            "Epoch 201, Perplexity: 1.07\n",
            "Epoch 202, Perplexity: 1.07\n",
            "Epoch 203, Perplexity: 1.07\n",
            "Epoch 204, Perplexity: 1.07\n",
            "Epoch 205, Perplexity: 1.07\n",
            "Epoch 206, Perplexity: 1.07\n",
            "Epoch 207, Perplexity: 1.07\n",
            "Epoch 208, Perplexity: 1.07\n",
            "Epoch 209, Perplexity: 1.07\n",
            "Epoch 210, Perplexity: 1.07\n",
            "Epoch 211, Perplexity: 1.07\n",
            "Epoch 212, Perplexity: 1.07\n",
            "Epoch 213, Perplexity: 1.07\n",
            "Epoch 214, Perplexity: 1.07\n",
            "Epoch 215, Perplexity: 1.07\n",
            "Epoch 216, Perplexity: 1.07\n",
            "Epoch 217, Perplexity: 1.07\n",
            "Epoch 218, Perplexity: 1.07\n",
            "Epoch 219, Perplexity: 1.07\n",
            "Epoch 220, Perplexity: 1.07\n",
            "Epoch 221, Perplexity: 1.07\n",
            "Epoch 222, Perplexity: 1.07\n",
            "Epoch 223, Perplexity: 1.07\n",
            "Epoch 224, Perplexity: 1.07\n",
            "Epoch 225, Perplexity: 1.07\n",
            "Epoch 226, Perplexity: 1.07\n",
            "Epoch 227, Perplexity: 1.07\n",
            "Epoch 228, Perplexity: 1.07\n",
            "Epoch 229, Perplexity: 1.07\n",
            "Epoch 230, Perplexity: 1.07\n",
            "Epoch 231, Perplexity: 1.07\n",
            "Epoch 232, Perplexity: 1.07\n",
            "Epoch 233, Perplexity: 1.07\n",
            "Epoch 234, Perplexity: 1.07\n",
            "Epoch 235, Perplexity: 1.07\n",
            "Epoch 236, Perplexity: 1.07\n",
            "Epoch 237, Perplexity: 1.07\n",
            "Epoch 238, Perplexity: 1.07\n",
            "Epoch 239, Perplexity: 1.07\n",
            "Epoch 240, Perplexity: 1.07\n",
            "Epoch 241, Perplexity: 1.07\n",
            "Epoch 242, Perplexity: 1.07\n",
            "Epoch 243, Perplexity: 1.07\n",
            "Epoch 244, Perplexity: 1.07\n",
            "Epoch 245, Perplexity: 1.07\n",
            "Epoch 246, Perplexity: 1.07\n",
            "Epoch 247, Perplexity: 1.07\n",
            "Epoch 248, Perplexity: 1.07\n",
            "Epoch 249, Perplexity: 1.07\n",
            "Epoch 250, Perplexity: 1.07\n",
            "Epoch 251, Perplexity: 1.07\n",
            "Epoch 252, Perplexity: 1.07\n",
            "Epoch 253, Perplexity: 1.07\n",
            "Epoch 254, Perplexity: 1.07\n",
            "Epoch 255, Perplexity: 1.07\n",
            "Epoch 256, Perplexity: 1.07\n",
            "Epoch 257, Perplexity: 1.07\n",
            "Epoch 258, Perplexity: 1.07\n",
            "Epoch 259, Perplexity: 1.07\n",
            "Epoch 260, Perplexity: 1.07\n",
            "Epoch 261, Perplexity: 1.07\n",
            "Epoch 262, Perplexity: 1.06\n",
            "Epoch 263, Perplexity: 1.06\n",
            "Epoch 264, Perplexity: 1.06\n",
            "Epoch 265, Perplexity: 1.06\n",
            "Epoch 266, Perplexity: 1.06\n",
            "Epoch 267, Perplexity: 1.06\n",
            "Epoch 268, Perplexity: 1.06\n",
            "Epoch 269, Perplexity: 1.06\n",
            "Epoch 270, Perplexity: 1.06\n",
            "Epoch 271, Perplexity: 1.06\n",
            "Epoch 272, Perplexity: 1.06\n",
            "Epoch 273, Perplexity: 1.06\n",
            "Epoch 274, Perplexity: 1.06\n",
            "Epoch 275, Perplexity: 1.06\n",
            "Epoch 276, Perplexity: 1.06\n",
            "Epoch 277, Perplexity: 1.06\n",
            "Epoch 278, Perplexity: 1.06\n",
            "Epoch 279, Perplexity: 1.06\n",
            "Epoch 280, Perplexity: 1.06\n",
            "Epoch 281, Perplexity: 1.06\n",
            "Epoch 282, Perplexity: 1.06\n",
            "Epoch 283, Perplexity: 1.06\n",
            "Epoch 284, Perplexity: 1.06\n",
            "Epoch 285, Perplexity: 1.06\n",
            "Epoch 286, Perplexity: 1.06\n",
            "Epoch 287, Perplexity: 1.06\n",
            "Epoch 288, Perplexity: 1.06\n",
            "Epoch 289, Perplexity: 1.06\n",
            "Epoch 290, Perplexity: 1.06\n",
            "Epoch 291, Perplexity: 1.06\n",
            "Epoch 292, Perplexity: 1.06\n",
            "Epoch 293, Perplexity: 1.06\n",
            "Epoch 294, Perplexity: 1.06\n",
            "Epoch 295, Perplexity: 1.06\n",
            "Epoch 296, Perplexity: 1.06\n",
            "Epoch 297, Perplexity: 1.06\n",
            "Epoch 298, Perplexity: 1.06\n",
            "Epoch 299, Perplexity: 1.06\n",
            "Epoch 300, Perplexity: 1.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.predict('the', 20, data.vocab, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "db9S9sI6KXUk",
        "outputId": "c9eac7e2-a4ed-40ca-c91a-995db55cd9b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thehehehehehehehehehehe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}