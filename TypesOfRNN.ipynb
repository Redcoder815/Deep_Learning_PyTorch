{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/TypesOfRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-to-One RNN\n"
      ],
      "metadata": {
        "id": "3MuJtXr7NYc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ----- Model Definition -----\n",
        "class SimpleRNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=10,   # matches last dimension of X\n",
        "            hidden_size=32,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)        # out: (batch, seq_len, hidden)\n",
        "        out = out[:, -1, :]         # take last timestep\n",
        "        out = self.fc(out)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# ----- Data -----\n",
        "X = torch.rand(1000, 1, 10)        # same shape as TF: (batch, seq, features)\n",
        "y = torch.randint(0, 2, (1000, 1)).float()\n",
        "\n",
        "# ----- Model, Loss, Optimizer -----\n",
        "model = SimpleRNNModel()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# ----- Training Loop -----\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    permutation = torch.randperm(X.size(0))\n",
        "\n",
        "    for i in range(0, X.size(0), batch_size):\n",
        "        idx = permutation[i:i+batch_size]\n",
        "        batch_x = X[idx]\n",
        "        batch_y = y[idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # optional: print progress\n",
        "    # print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "7VU63hKe7q6C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-to-Many RNN\n"
      ],
      "metadata": {
        "id": "JYPD9CxEPy6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# One-to-Many RNN model in PyTorch\n",
        "class OneToManyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(2048, 64)          # Dense layer\n",
        "        self.repeat = 20                       # RepeatVector(20)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=64,\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            nonlinearity='tanh'\n",
        "        )\n",
        "        self.out = nn.Linear(128, 10000)       # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 2048)\n",
        "        x = self.fc(x)                         # (batch, 64)\n",
        "        x = x.unsqueeze(1).repeat(1, self.repeat, 1)  # (batch, 20, 64)\n",
        "        x, _ = self.rnn(x)                     # (batch, 20, 128)\n",
        "        x = self.out(x)                        # (batch, 20, 10000)\n",
        "        # return torch.softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "model = OneToManyRNN()\n",
        "\n",
        "# Loss + optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Simulated data\n",
        "X = torch.rand(500, 2048)\n",
        "y = torch.rand(500, 20, 10000)\n",
        "\n",
        "# PyTorch expects class indices for CrossEntropyLoss,\n",
        "# but your TF code uses one-hot softmax targets.\n",
        "# So we convert to class indices:\n",
        "y_indices = torch.argmax(y, dim=-1)   # (500, 20)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        yb = y_indices[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)                     # (batch, 20, 10000)\n",
        "\n",
        "        # CrossEntropyLoss expects (batch*seq, classes)\n",
        "        loss = criterion(\n",
        "            preds.reshape(-1, 10000),\n",
        "            yb.reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, loss={loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sliP_A7PhdU",
        "outputId": "82744111-551f-4128-e667-c4e7db29e7bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss=9.2566\n",
            "Epoch 2, loss=9.3040\n",
            "Epoch 3, loss=8.4692\n",
            "Epoch 4, loss=8.1023\n",
            "Epoch 5, loss=7.9306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many-to-One RNN\n"
      ],
      "metadata": {
        "id": "S1JYNX4na0RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Model using your forward()\n",
        "# -----------------------------\n",
        "class ManyToOneRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=128,\n",
        "            hidden_size=64,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(64, 32)     # Dense(32, relu)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out1 = nn.Linear(32, 3)    # Dense(3, softmax) â†’ logits only\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)            # (batch, seq, hidden)\n",
        "        out = out[:, -1, :]             # last timestep\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.out1(out)            # raw logits\n",
        "        return out                      # CrossEntropyLoss expects logits\n",
        "\n",
        "# -----------------------------\n",
        "# Random data like your TF code\n",
        "# -----------------------------\n",
        "X = np.random.rand(2000, 100, 128).astype(np.float32)\n",
        "y = np.random.randint(0, 3, 2000)\n",
        "\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# -----------------------------\n",
        "# Training setup\n",
        "# -----------------------------\n",
        "model = ManyToOneRNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop\n",
        "# -----------------------------\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    permutation = torch.randperm(X.size(0))\n",
        "\n",
        "    for i in range(0, X.size(0), batch_size):\n",
        "        idx = permutation[i:i+batch_size]\n",
        "        batch_x = X[idx]\n",
        "        batch_y = y[idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_x)\n",
        "        loss = criterion(logits, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "lp4PAW7naoBU",
        "outputId": "3aaee7c9-f73a-4c2d-fa99-96ed033b21b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 1.1063\n",
            "Epoch 2/10 - Loss: 1.0941\n",
            "Epoch 3/10 - Loss: 1.0934\n",
            "Epoch 4/10 - Loss: 1.1029\n",
            "Epoch 5/10 - Loss: 1.0876\n",
            "Epoch 6/10 - Loss: 1.0976\n",
            "Epoch 7/10 - Loss: 1.0678\n",
            "Epoch 8/10 - Loss: 1.0533\n",
            "Epoch 9/10 - Loss: 0.9004\n",
            "Epoch 10/10 - Loss: 0.8767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many-to-Many RNN"
      ],
      "metadata": {
        "id": "Hj47nPC-fRSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ManyToManyRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        # x: (batch, seq_len, input_size)\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        # out: (batch, seq_len, hidden_size)\n",
        "        logits = self.fc(out)\n",
        "        # logits: (batch, seq_len, output_size)\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "batch_size = 4\n",
        "seq_len = 6\n",
        "input_size = 10\n",
        "hidden_size = 32\n",
        "output_size = 8\n",
        "\n",
        "model = ManyToManyRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Fake data\n",
        "x = torch.randn(batch_size, seq_len, input_size)\n",
        "y = torch.randn(batch_size, seq_len, output_size)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    logits, _ = model(x)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"step {step}, loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "RWuoza3afNAM",
        "outputId": "4f55433f-3f98-4854-c4a2-72de9617a18a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, loss = 0.7726\n",
            "step 50, loss = 0.4434\n",
            "step 100, loss = 0.1895\n",
            "step 150, loss = 0.0658\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}