{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/00Preliminaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "v2ELTsbnTHzU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "a, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTaTLCh9TBuj",
        "outputId": "3568d8d7-631f-4ac0-acbb-4c73b8e872f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]),\n",
              " tensor([[0, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R3fJKIdTEH3",
        "outputId": "1b8a9a4d-266a-4b1c-87b7-e97f112ecbdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
        "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('''NumRooms,RoofType,Price\n",
        "NA,NA,127500\n",
        "2,NA,106000\n",
        "4,Slate,178100\n",
        "NA,NA,140000''')"
      ],
      "metadata": {
        "id": "iBwhpTfjZIKM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z267-ZrgZS9L",
        "outputId": "8c1ed87a-8154-4272-80c5-01409a5e06e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms RoofType   Price\n",
            "0       NaN      NaN  127500\n",
            "1       2.0      NaN  106000\n",
            "2       4.0    Slate  178100\n",
            "3       NaN      NaN  140000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rftw0JuCbRpK",
        "outputId": "149bc771-a344-45a6-c385-4715241a37af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms  RoofType_Slate  RoofType_nan\n",
            "0       NaN           False          True\n",
            "1       2.0           False          True\n",
            "2       4.0            True         False\n",
            "3       NaN           False          True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(6).reshape(6, 1)\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj9zlemsq_1F",
        "outputId": "9890757a-2230-46ae-a0c6-f972404d8bef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [1],\n",
              "        [2],\n",
              "        [3],\n",
              "        [4],\n",
              "        [5]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "a + X, (a * X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfoz8upfwEQo",
        "outputId": "0c0bd145-029b-49c9-fdad-9c50a7dd1cb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2,  3,  4,  5],\n",
              "          [ 6,  7,  8,  9],\n",
              "          [10, 11, 12, 13]],\n",
              " \n",
              "         [[14, 15, 16, 17],\n",
              "          [18, 19, 20, 21],\n",
              "          [22, 23, 24, 25]]]),\n",
              " tensor([[[ 0,  2,  4,  6],\n",
              "          [ 8, 10, 12, 14],\n",
              "          [16, 18, 20, 22]],\n",
              " \n",
              "         [[24, 26, 28, 30],\n",
              "          [32, 34, 36, 38],\n",
              "          [40, 42, 44, 46]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
        "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
        "A, A + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWxuf2Djyv0z",
        "outputId": "1d648b8a-9bf2-4e7d-a5ab-4925e67dd44f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]),\n",
              " tensor([[ 0.,  2.,  4.],\n",
              "         [ 6.,  8., 10.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, A.sum(axis=0).shape, A.sum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXKlsTNWy3OB",
        "outputId": "730b91e6-0401-4d54-a7f3-55106b84fab0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), torch.Size([3]), tensor([3., 5., 7.]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, A.sum(axis=1).shape, A.sum(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR3L57nszJK0",
        "outputId": "ff25068c-eb71-4c7e-dc3d-05afadbdb2a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), torch.Size([2]), tensor([ 3., 12.]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.sum(axis=[0, 1])  # Same as A.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4TOHAXb0JAO",
        "outputId": "4b528507-fa33-4a3b-ef2a-788f47e10cf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15.)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "c8smCU8VHj3b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a function f for which you want to calculate the numerical limit.\n",
        "# For example, let's define f(x) = x**2\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "for h in 10.0**np.arange(-1, -6, -1):\n",
        "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRpVUeaiHgYl",
        "outputId": "db2b35ee-0ad8-4cd4-d256-ef1a950c3810"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h=0.10000, numerical limit=2.10000\n",
            "h=0.01000, numerical limit=2.01000\n",
            "h=0.00100, numerical limit=2.00100\n",
            "h=0.00010, numerical limit=2.00010\n",
            "h=0.00001, numerical limit=2.00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34wQiWUmQXk0",
        "outputId": "1cfe4307-75d8-4395-d7af-85314b31f7d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
        "x.requires_grad_(True)\n",
        "x.grad  # The gradient is None by default"
      ],
      "metadata": {
        "id": "uGpVfvuTQabo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9x4gRQGQdcJ",
        "outputId": "ca334736-cabf-49be-926a-024324f64cbb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of x.grad is calculated through PyTorch's automatic differentiation mechanism, specifically after calling y.backward(). This operation computes the gradient of y with respect to x.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "Define x and y:\n",
        "\n",
        "x = torch.arange(4.0) results in x = tensor([0., 1., 2., 3.]).\n",
        "y = 2 * torch.dot(x, x)\n",
        "Expand y: If x = [x_0, x_1, x_2, x_3], then torch.dot(x, x) is x_0^2 + x_1^2 + x_2^2 + x_3^2. So, y = 2 * (x_0^2 + x_1^2 + x_2^2 + x_3^2).\n",
        "\n",
        "Calculate the Partial Derivatives (Gradients): To get x.grad, PyTorch computes the partial derivative of y with respect to each component of x (dy/dx_i):\n",
        "\n",
        "dy/dx_0 = d/dx_0 [2 * (x_0^2 + x_1^2 + x_2^2 + x_3^2)] = 2 * (2 * x_0) = 4 * x_0\n",
        "dy/dx_1 = d/dx_1 [2 * (x_0^2 + x_1^2 + x_2^2 + x_3^2)] = 2 * (2 * x_1) = 4 * x_1\n",
        "dy/dx_2 = d/dx_2 [2 * (x_0^2 + x_1^2 + x_2^2 + x_3^2)] = 2 * (2 * x_2) = 4 * x_2\n",
        "dy/dx_3 = d/dx_3 [2 * (x_0^2 + x_1^2 + x_2^2 + x_3^2)] = 2 * (2 * x_3) = 4 * x_3\n",
        "Substitute x values: Now, substitute the values from x = tensor([0., 1., 2., 3.]) into these derivatives:\n",
        "\n",
        "4 * 0. = 0.\n",
        "4 * 1. = 4.\n",
        "4 * 2. = 8.\n",
        "4 * 3. = 12.\n",
        "This results in x.grad being tensor([0., 4., 8., 12.])."
      ],
      "metadata": {
        "id": "9PeCcPNISY2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D9_xUdLRL1F",
        "outputId": "b8f703f3-1ff4-4bcd-f7ed-dfb4fe79f7d9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad == 4 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgyEuxFlSi71",
        "outputId": "42589372-6dcc-44c9-8cf3-36e7e0b2da6e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The x.grad value is [1., 1., 1., 1.] in the last block because of the following steps:\n",
        "\n",
        "x.grad.zero_(): This line explicitly resets the gradients accumulated from previous backward() calls to zero. Before this, x.grad was [0., 4., 8., 12.].\n",
        "\n",
        "y = x.sum(): The variable y is redefined to be the sum of all elements in x.\n",
        "\n",
        "x = tensor([0., 1., 2., 3.])\n",
        "So, y = 0. + 1. + 2. + 3. = 6.\n",
        "y.backward(): This triggers the computation of gradients of the new y (which is x.sum()) with respect to x. The chain rule is applied.\n",
        "\n",
        "If y = x_0 + x_1 + x_2 + x_3,\n",
        "Then the partial derivative dy/dx_0 = 1\n",
        "dy/dx_1 = 1\n",
        "dy/dx_2 = 1\n",
        "dy/dx_3 = 1\n",
        "Since x.grad was reset to zero before this backward() call, the newly computed gradients ([1., 1., 1., 1.]) are assigned to x.grad."
      ],
      "metadata": {
        "id": "YmannpOTTnU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdWQ8B16TQeF",
        "outputId": "43f90ef8-7e03-4078-b13e-71428db03c4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down how x.grad became [0., 2., 4., 6.] in that last code block:\n",
        "\n",
        "x.grad.zero_(): First, any previously accumulated gradients in x.grad are reset to zero. This ensures that we are calculating fresh gradients for the new y definition.\n",
        "\n",
        "y = x * x: Here, y is redefined. Unlike the previous examples where y was a scalar (2 * torch.dot(x,x) or x.sum()), this operation performs an element-wise multiplication. So, y becomes a tensor of the same shape as x:\n",
        "\n",
        "x = tensor([0., 1., 2., 3.])\n",
        "y[0] = 0. * 0. = 0.\n",
        "y[1] = 1. * 1. = 1.\n",
        "y[2] = 2. * 2. = 4.\n",
        "y[3] = 3. * 3. = 9. So, y = tensor([0., 1., 4., 9.]).\n",
        "y.backward(gradient=torch.ones(len(y))): When y is a non-scalar tensor (like a vector or matrix), y.backward() requires a gradient argument. This argument is a tensor of the same shape as y, and it represents the gradient of the 'overall output' (which y contributes to) with respect to y.\n",
        "\n",
        "In this case, gradient=torch.ones(len(y)) means gradient=tensor([1., 1., 1., 1.]). This effectively tells PyTorch to consider the sum of y as the final scalar output for which we want gradients.\n",
        "For each element y_i = x_i^2, the derivative dy_i/dx_i is 2 * x_i.\n",
        "Since the gradient argument is [1., 1., 1., 1.], the final gradient x.grad_i is effectively 1 * (2 * x_i).\n",
        "Let's apply this:\n",
        "\n",
        "For x[0] = 0.: x.grad[0] = 2 * 0. = 0.\n",
        "For x[1] = 1.: x.grad[1] = 2 * 1. = 2.\n",
        "For x[2] = 2.: x.grad[2] = 2 * 2. = 4.\n",
        "For x[3] = 3.: x.grad[3] = 2 * 3. = 6.\n",
        "This results in x.grad = tensor([0., 2., 4., 6.]).\n",
        "\n",
        "The comment # Faster: y.sum().backward() is a shortcut that achieves the same result, as y.sum().backward() implicitly uses torch.ones_like(y) as the gradient argument."
      ],
      "metadata": {
        "id": "gW7E-a2uV8u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EUsmVvuU-uG",
        "outputId": "de44e949-4170-44f7-e56b-735bff39892e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The .detach() method in PyTorch is used to create a new tensor that shares the same storage with the original tensor but does not track its history of operations. In simple terms, it removes the tensor from the computational graph, making it a constant for subsequent operations with respect to gradient calculations.\n",
        "\n",
        "Let's break it down in the context of your code:\n",
        "\n",
        "y = x * x: Here, y is computed based on x, so PyTorch tracks this operation to compute gradients later.\n",
        "u = y.detach(): This is where detach() comes in.\n",
        "u gets the value of y (which is tensor([0., 1., 4., 9.])).\n",
        "However, u is detached from the computational graph that connects y back to x. This means that any operation involving u will treat u as a constant, not as something whose value depends on x through y.\n",
        "z = u * x: Now, z is calculated by multiplying u and x element-wise.\n",
        "When z.sum().backward() is called, PyTorch calculates the gradients of z.sum() with respect to x.\n",
        "Since u was detached, it's treated as a constant during this gradient calculation. So, if z_i = u_i * x_i, then the derivative dz_i/dx_i is simply u_i (because u_i is treated as a constant coefficient).\n",
        "Therefore, after z.sum().backward(), x.grad will be equal to u because u acts as the gradient of z with respect to x when u itself is not considered a function of x."
      ],
      "metadata": {
        "id": "dQxXIyuTYVJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEI8reVQXqxd",
        "outputId": "d979386c-0d51-4716-c65a-ac2cf2c56f97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VCe1M6yYcE-",
        "outputId": "5efb723c-e12f-4285-e49c-def0ccecfad8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "metadata": {
        "id": "_qw2ZCAVdCCn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "metadata": {
        "id": "Yc7uGCTbdFQS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad, a.grad == d / a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvJ1WgOqdIAX",
        "outputId": "5b00b2b4-e009-4283-fce5-0eb89f0488af"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(102400.), tensor(True))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probability"
      ],
      "metadata": {
        "id": "Khsb-ei6f6oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.distributions.multinomial import Multinomial"
      ],
      "metadata": {
        "id": "wJBLgVNdgR4i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tosses = 100\n",
        "heads = sum([random.random() > 0.5 for _ in range(num_tosses)])\n",
        "tails = num_tosses - heads\n",
        "print(\"heads, tails: \", [heads, tails])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unwglFVof_AS",
        "outputId": "ab8c42a4-b669-474c-ccf6-1b22e4513e3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads, tails:  [57, 43]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down Multinomial(100, fair_probs).sample() with an everyday example.\n",
        "\n",
        "Imagine you're flipping a fair coin 100 times.\n",
        "\n",
        "Multinomial: This refers to a [redacted link]. It's a generalization of the binomial distribution, used when there are more than two possible outcomes (like rolling a multi-sided die) or for multiple trials of a binary outcome (like many coin flips).\n",
        "\n",
        "100: This is the total_count parameter. In our coin-flipping example, this is the total number of times you're going to flip the coin (100 flips).\n",
        "\n",
        "fair_probs = torch.tensor([0.5, 0.5]): This is the probs parameter, representing the probabilities of each possible outcome. Since fair_probs is [0.5, 0.5], it means:\n",
        "\n",
        "The first outcome (let's say 'Heads') has a 50% chance (0.5).\n",
        "The second outcome (let's say 'Tails') also has a 50% chance (0.5). This perfectly describes a fair coin.\n",
        ".sample(): This method tells the distribution to generate one random outcome based on the given probabilities and total count.\n",
        "\n",
        "Putting it together (Example):\n",
        "\n",
        "When you run Multinomial(100, fair_probs).sample(), PyTorch simulates 100 fair coin flips and then tells you how many times each outcome ('Heads' and 'Tails') occurred.\n",
        "\n",
        "For instance, the output tensor([46., 54.]) from your notebook means that out of 100 coin flips, the simulation resulted in:\n",
        "\n",
        "46 'Heads' (the first element, corresponding to the first probability in fair_probs)\n",
        "54 'Tails' (the second element, corresponding to the second probability in fair_probs)\n",
        "Notice that the sum 46 + 54 = 100, which equals the total_count you specified. Each time you call .sample(), you'll likely get slightly different numbers (e.g., [52, 48], [49, 51]), but their sum will always be 100."
      ],
      "metadata": {
        "id": "HIGImpXXiL-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fair_probs = torch.tensor([0.5, 0.5])\n",
        "Multinomial(100, fair_probs).sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX1ZJA05g9sp",
        "outputId": "e26499ea-d5a6-44a0-89ad-1ca8e1a3a863"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([46., 54.])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Multinomial(100, fair_probs).sample() / 100"
      ],
      "metadata": {
        "id": "3-my4Ro1ikra",
        "outputId": "f8d849af-a733-48fa-d57e-9af35bf9d6bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5600, 0.4400])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = Multinomial(10000, fair_probs).sample()\n",
        "counts / 10000"
      ],
      "metadata": {
        "id": "dpaoKPhQi_j-",
        "outputId": "a703bb71-fd3e-4702-a4f2-70a7538be9af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4987, 0.5013])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}