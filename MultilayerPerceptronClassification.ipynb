{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQyUy5opPBMwAQNEJLQUo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_PyTorch/blob/main/MultilayerPerceptronClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, layer_sizes, activation=F.relu):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "\n",
        "        self.weights = nn.ParameterList()\n",
        "        self.biases = nn.ParameterList()\n",
        "\n",
        "        for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            W = nn.Parameter(torch.randn(in_dim, out_dim) * 0.01)\n",
        "            b = nn.Parameter(torch.zeros(out_dim))\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            x = x @ W + b\n",
        "            if i < len(self.weights) - 1:\n",
        "                x = self.activation(x)\n",
        "        return x  # logits\n",
        "\n",
        "# --- Your MLP Class remains the same ---\n",
        "# (Assumed to be defined above)\n",
        "\n",
        "# 1. Generate Synthetic Data (Non-linear 'Moons' dataset)\n",
        "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1) # Shape (N, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# 2. Initialize Model, Optimizer, and Loss\n",
        "model = MLP([2, 16, 16, 1]) # Added an extra layer for better capacity\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 3. Training Loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(X_train)\n",
        "    loss = loss_fn(logits, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        # Calculate accuracy\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Apply sigmoid to get probabilities, then round to 0 or 1\n",
        "            preds = torch.round(torch.sigmoid(model(X_test)))\n",
        "            acc = (preds == y_test).float().mean()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Test Acc: {acc.item():.4f}\")\n",
        "\n",
        "# 4. Final Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    final_logits = model(X_test)\n",
        "    final_preds = torch.round(torch.sigmoid(final_logits))\n",
        "    final_acc = (final_preds == y_test).float().mean()\n",
        "    print(f\"\\nFinal Test Accuracy: {final_acc.item() * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M_2TDGqymwL",
        "outputId": "7a633698-ec76-4290-b07f-b3987d4908aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/500 | Loss: 0.2528 | Test Acc: 0.8700\n",
            "Epoch 200/500 | Loss: 0.2527 | Test Acc: 0.8700\n",
            "Epoch 300/500 | Loss: 0.2527 | Test Acc: 0.8700\n",
            "Epoch 400/500 | Loss: 0.2527 | Test Acc: 0.8700\n",
            "Epoch 500/500 | Loss: 0.2527 | Test Acc: 0.8700\n",
            "\n",
            "Final Test Accuracy: 87.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, layer_sizes, activation=F.relu):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "\n",
        "        self.weights = nn.ParameterList()\n",
        "        self.biases = nn.ParameterList()\n",
        "\n",
        "        for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            W = nn.Parameter(torch.randn(in_dim, out_dim) * 0.01)\n",
        "            b = nn.Parameter(torch.zeros(out_dim))\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            x = x @ W + b\n",
        "            if i < len(self.weights) - 1:\n",
        "                x = self.activation(x)\n",
        "        return x  # logits\n",
        "\n",
        "# 1. Generate Synthetic Multi-class Data (3 classes)\n",
        "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=1.0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long) # CrossEntropyLoss expects Long/int labels\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 2. Initialize Model (2 features -> 16 hidden -> 3 classes)\n",
        "model = MLP([2, 16, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Training Loop\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass: model returns raw logits (batch_size, 3)\n",
        "    logits = model(X_train)\n",
        "    loss = loss_fn(logits, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate every 50 epochs\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Accuracy calculation using argmax to find the predicted class\n",
        "            test_logits = model(X_test)\n",
        "            preds = torch.argmax(test_logits, dim=1)\n",
        "            acc = (preds == y_test).float().mean()\n",
        "            print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Test Acc: {acc.item():.4f}\")\n",
        "\n",
        "# 4. Final Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    final_logits = model(X_test)\n",
        "    final_preds = torch.argmax(final_logits, dim=1)\n",
        "    final_acc = (final_preds == y_test).float().mean()\n",
        "    print(f\"\\nFinal Test Accuracy: {final_acc.item() * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKas6GzizMzs",
        "outputId": "f6252fdf-1f87-4f12-e326-5619120a6e65"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  50 | Loss: 0.0009 | Test Acc: 1.0000\n",
            "Epoch 100 | Loss: 0.0005 | Test Acc: 1.0000\n",
            "Epoch 150 | Loss: 0.0004 | Test Acc: 1.0000\n",
            "Epoch 200 | Loss: 0.0003 | Test Acc: 1.0000\n",
            "\n",
            "Final Test Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}